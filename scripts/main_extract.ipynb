{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63b0cd8-f07a-4d08-917e-e19168fe12d7",
   "metadata": {},
   "source": [
    "### xxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96df4ba-a813-4e82-a36e-4149963bd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Pakete importieren\n",
    "# \"Standard\"-import\n",
    "import os\n",
    "import pandas as pd\n",
    "# Einlesen & entpacken in html-Dateien\n",
    "from glob import glob\n",
    "import tarfile\n",
    "# Kreation Mini-Data-Warehouse (SQL-Datenbank)\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "# Bearbeiten von html-Dateien\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9966097-f518-4e15-a890-ce37773747dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade für das Projekt \n",
    "\n",
    "# Pfad für das Projekt\n",
    "PROJECT_ROOT = os.getcwd()  \n",
    "\n",
    "# Input, Rohdaten & html-Dateien\n",
    "INPUT_PATH = os.path.join(PROJECT_ROOT, \"input\", \"raw\")\n",
    "ZIP_PATH = os.path.join(INPUT_PATH, \"downloaded_zips\")\n",
    "DATA_LAKE_PATH = os.path.join(INPUT_PATH, \"data-lake\")  \n",
    "\n",
    "# Mini-Data-Warehouse bzw. SQL-Datenbank\n",
    "LOGFILE_PATH = os.path.join(DATA_LAKE_PATH, \"2025-02-08.csv\")\n",
    "SQL_PATH = os.path.join(PROJECT_ROOT, \"output\", \"dwh.sqlite3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7a4d13-fadf-49c1-8e49-e315bd3b314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahieren & Einlesen einer .tar.gz-Datei\n",
    "def extract_tar_file(ZIP_PATH, DATA_LAKE_PATH): \n",
    "    with tarfile.open(ZIP_PATH, \"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            # Zielpfad für jede Datei bestimmen\n",
    "            filename = os.path.basename(member.name)\n",
    "            \n",
    "            # Nur HTML-Dateien entpacken\n",
    "            if filename.endswith(\".html\"):\n",
    "                target_path = os.path.join(DATA_LAKE_PATH, filename)\n",
    "                \n",
    "                # Skippen, wenn Datei schon existiert\n",
    "                if os.path.exists(target_path):\n",
    "                    continue\n",
    "                \n",
    "                # Entpacken in HTML-Datei\n",
    "                with open(target_path, \"wb\") as f:\n",
    "                    f.write(tar.extractfile(member).read())\n",
    "                \n",
    "                print(f\"Entpackt: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c517cf6b-8ac3-4333-bce7-8d70fd6c7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html-Datei mit Encoding einlesen\n",
    "def read_html_file(filepath, encoding=\"utf-8\"):\n",
    "    # Datei öffnen und einlesen\n",
    "    with open(filepath, \"r\", encoding=encoding) as f:\n",
    "        return f.read()\n",
    "\n",
    "# html-Inhalt bereinigen & in einzelne Wörter zerlegen\n",
    "def process_html(html):\n",
    "    # html-Parser von BeautifulSoup nutzen, um nur den Text auszulesen\n",
    "    bstext = BeautifulSoup(html, \"html.parser\")\n",
    "    text = bstext.get_text(separator=\" \").lower()\n",
    "\n",
    "    # Zeilenumbrüche durch Leerzeichen ersetzen & Wörter trennen\n",
    "    items = text.replace(\"\\n\", \" \").split(\" \")\n",
    "\n",
    "    # Stoppwörter importieren\n",
    "    stopwords_url = \"https://raw.githubusercontent.com/solariz/german_stopwords/master/german_stopwords_full.txt\"\n",
    "    now_str = \"2025-02-08\"\n",
    "    stopwords_list = requests.get(stopwords_url, allow_redirects=True).text.split(\"\\n\")[9:]\n",
    "\n",
    "    # Kurze Wörter und Stoppwörter entfernen\n",
    "    items = [i for i in items if len(i) > 1 and i not in stopwords_list]\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd5b43ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m one_row = \u001b[43mlog_file\u001b[49m.iloc[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# oder log_file[log_file[\"name\"] == \"sz\"].iloc[0]\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(one_row)\n",
      "\u001b[31mNameError\u001b[39m: name 'log_file' is not defined"
     ]
    }
   ],
   "source": [
    "one_row = log_file.iloc[0]  # oder log_file[log_file[\"name\"] == \"sz\"].iloc[0]\n",
    "print(one_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad81fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta\\Documents\\Marta\\DBU\\ADSC11 ADS-01\\Studienarbeit\\newspaper-scraping\\input\\raw\\data-lake\\2025-02-08-sz.html\n",
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(DATA_LAKE_PATH, os.path.basename(one_row[\"file_name\"]))\n",
    "test_encoding = one_row[\"encoding\"]\n",
    "print(test_path)\n",
    "print(test_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0460f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"de\"><head><script type=\"text/javascript\">window.dataLayer = [{\"page.publisher\":\"sz\",\"page.platform_type\":\"standard\",\"page.brand_product\":\"sz_de\",\"page.object_id\":\"8\",\"page.docType\":\"homepage\",\"page.tech\":\"homie\",\"tech.robots_status\":\"true\",\"ads.adtag\":\"true\"}]</script><meta data-meta-viewport=\"true\" name=\"viewport\" content=\"width=device-width, initial-scale=1\"/>\n",
      "  <!--\n",
      "        ____               _      __           ___      __       __         _        __             _\n"
     ]
    }
   ],
   "source": [
    "html = read_html_file(test_path, test_encoding)\n",
    "print(html[:500])  # Nur die ersten 500 Zeichen anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7453595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einzelne Zeitungsdateien verarbeiten \n",
    "def process_newspaper(newspaper):\n",
    "    filename = os.path.basename(newspaper[\"file_name\"])  # Nur den Dateinamen rausholen\n",
    "    full_path = os.path.join(DATA_LAKE_PATH, filename)   \n",
    "\n",
    "    # Encoding aus der CSV einlesen\n",
    "    encoding = newspaper[\"encoding\"].lower()\n",
    "    \n",
    "    # html-Datei laden & bereinigen\n",
    "    html = read_html_file(full_path, encoding)\n",
    "    items = process_html(html)\n",
    "\n",
    "   # Häufigkeit jedes Wortes zählen\n",
    "    count = pd.Series(items).value_counts()\n",
    "\n",
    "    # Als DataFrame formatieren und Zusatzinfos ergänzen\n",
    "    count_df = count.to_frame()\n",
    "    count_df.columns = [\"count\"]\n",
    "    count_df[\"word\"] = count_df.index\n",
    "    count_df[\"paper\"] = newspaper[\"name\"]\n",
    "    count_df[\"date\"] = newspaper[\"date\"]\n",
    "\n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c575967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                                 0\n",
      "name                                      sz\n",
      "date                              2025-02-08\n",
      "file_name       data-lake/2025-02-08-sz.html\n",
      "status                                   200\n",
      "original_url    https://www.sueddeutsche.de/\n",
      "final_url       https://www.sueddeutsche.de/\n",
      "encoding                               utf-8\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "one_row = log_file.iloc[0]\n",
    "print(one_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c6ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>word</th>\n",
       "      <th>paper</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sz</th>\n",
       "      <td>142</td>\n",
       "      <td>sz</td>\n",
       "      <td>sz</td>\n",
       "      <td>2025-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plus</th>\n",
       "      <td>121</td>\n",
       "      <td>plus</td>\n",
       "      <td>sz</td>\n",
       "      <td>2025-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anzeigen</th>\n",
       "      <td>26</td>\n",
       "      <td>anzeigen</td>\n",
       "      <td>sz</td>\n",
       "      <td>2025-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magazin</th>\n",
       "      <td>10</td>\n",
       "      <td>magazin</td>\n",
       "      <td>sz</td>\n",
       "      <td>2025-02-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artikel</th>\n",
       "      <td>9</td>\n",
       "      <td>artikel</td>\n",
       "      <td>sz</td>\n",
       "      <td>2025-02-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count      word paper        date\n",
       "sz          142        sz    sz  2025-02-08\n",
       "plus        121      plus    sz  2025-02-08\n",
       "anzeigen     26  anzeigen    sz  2025-02-08\n",
       "magazin      10   magazin    sz  2025-02-08\n",
       "artikel       9   artikel    sz  2025-02-08"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = process_newspaper(one_row)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed4b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste für die Ergebnisse-Sammlung erstellen\n",
    "collection = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70183c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verarbeitung jeder einzelnen Datei und DataFrame mit Wortzählungen zurückgeben\n",
    "def process_wrapper(newspaper):\n",
    "    name = newspaper[\"name\"]\n",
    "    try:\n",
    "        count = process_newspaper(newspaper)\n",
    "        print(f\"[INFO] Verarbeitung erfolgreich: {name}\")\n",
    "        collection.append(count)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Fehler bei {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229771e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Verarbeitung erfolgreich: sz\n",
      "[INFO] Verarbeitung erfolgreich: zeit\n",
      "[INFO] Verarbeitung erfolgreich: faz\n",
      "[INFO] Verarbeitung erfolgreich: heise\n",
      "[INFO] Verarbeitung erfolgreich: golem\n",
      "[INFO] Verarbeitung erfolgreich: tagesspiegel\n",
      "[INFO] Verarbeitung erfolgreich: taz\n",
      "[INFO] Verarbeitung erfolgreich: abendblatt\n",
      "[INFO] Verarbeitung erfolgreich: berliner\n",
      "[INFO] Verarbeitung erfolgreich: welt\n",
      "[INFO] Verarbeitung erfolgreich: medium\n",
      "[INFO] Verarbeitung erfolgreich: esslinger\n",
      "[INFO] Verarbeitung erfolgreich: kdnuggets\n",
      "[INFO] Verarbeitung erfolgreich: handelsblatt\n",
      "[INFO] Verarbeitung erfolgreich: ntv\n",
      "[INFO] Verarbeitung erfolgreich: pioneer\n",
      "[INFO] Verarbeitung erfolgreich: suedwest\n",
      "[INFO] Verarbeitung erfolgreich: t3n\n",
      "[INFO] Verarbeitung erfolgreich: economist\n",
      "[INFO] Verarbeitung erfolgreich: srf\n",
      "[INFO] Verarbeitung erfolgreich: wef\n",
      "[INFO] Verarbeitung erfolgreich: stuttgarter\n",
      "[INFO] Verarbeitung erfolgreich: atlantic\n",
      "[INFO] Verarbeitung erfolgreich: netzpolitik\n",
      "[INFO] Verarbeitung erfolgreich: towardsds\n",
      "[INFO] Verarbeitung erfolgreich: uebermedien\n",
      "[INFO] Verarbeitung erfolgreich: vulture\n",
      "[INFO] Verarbeitung erfolgreich: boerse\n",
      "[INFO] Verarbeitung erfolgreich: buchreport\n",
      "[INFO] Verarbeitung erfolgreich: 54books\n",
      "[INFO] Verarbeitung erfolgreich: dlf\n",
      "[INFO] Verarbeitung erfolgreich: dw-de\n",
      "[INFO] Verarbeitung erfolgreich: spiegel\n",
      "[INFO] Verarbeitung erfolgreich: cnn\n",
      "[INFO] Verarbeitung erfolgreich: bbc\n",
      "[INFO] Verarbeitung erfolgreich: dhv\n",
      "[INFO] Verarbeitung erfolgreich: mm\n",
      "[INFO] Verarbeitung erfolgreich: stern\n",
      "[INFO] Verarbeitung erfolgreich: tagesschau\n",
      "[INFO] Verarbeitung erfolgreich: zvw\n",
      "[INFO] Verarbeitung erfolgreich: ieee\n",
      "[INFO] Verarbeitung erfolgreich: anwaltsverein\n",
      "[INFO] Verarbeitung erfolgreich: wiwo\n",
      "[INFO] Verarbeitung erfolgreich: dw-en\n",
      "[INFO] Verarbeitung erfolgreich: dav\n",
      "[INFO] Verarbeitung erfolgreich: zwanzig\n",
      "[INFO] Verarbeitung erfolgreich: blick\n",
      "[INFO] Verarbeitung erfolgreich: nzz\n",
      "[INFO] Verarbeitung erfolgreich: republik\n",
      "[INFO] Verarbeitung erfolgreich: ta\n",
      "[INFO] Verarbeitung erfolgreich: watson-ch\n",
      "[INFO] Verarbeitung erfolgreich: nau\n",
      "[INFO] Verarbeitung erfolgreich: standard\n",
      "[INFO] Verarbeitung erfolgreich: kronen\n",
      "[INFO] Verarbeitung erfolgreich: kurier\n",
      "[INFO] Verarbeitung erfolgreich: kleine\n",
      "[INFO] Verarbeitung erfolgreich: watson-de\n",
      "[INFO] Verarbeitung erfolgreich: vice-de\n",
      "Data shape: (53939, 4)\n"
     ]
    }
   ],
   "source": [
    "# Alle CSV-Dateien im data-lake automatisch durchgehen\n",
    "csv_files = sorted([f for f in os.listdir(DATA_LAKE_PATH) if f.endswith(\".csv\")])\n",
    "\n",
    "for file in csv_files:\n",
    "    path = os.path.join(DATA_LAKE_PATH, file)\n",
    "    try:\n",
    "        log_file = pd.read_csv(path)\n",
    "        log_file.apply(process_wrapper, axis=1)\n",
    "        print(f\"[INFO] verarbeitet: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Fehler bei Datei {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df435d94",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Das System kann den angegebenen Pfad nicht finden: 'c:\\\\Users\\\\Marta\\\\Documents\\\\Marta\\\\DBU\\\\ADSC11 ADS-01\\\\Studienarbeit\\\\newspaper-scraping\\\\scripts\\\\input\\\\raw\\\\data-lake'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m csv_files = \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_LAKE_PATH\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f.endswith(\u001b[33m\"\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m)])\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: 'c:\\\\Users\\\\Marta\\\\Documents\\\\Marta\\\\DBU\\\\ADSC11 ADS-01\\\\Studienarbeit\\\\newspaper-scraping\\\\scripts\\\\input\\\\raw\\\\data-lake'"
     ]
    }
   ],
   "source": [
    "csv_files = sorted([f for f in os.listdir(DATA_LAKE_PATH) if f.endswith(\".csv\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02346ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Ergebnisse in einer Tabelle sammeln\n",
    "if collection:\n",
    "    data = pd.concat(collection, axis=0)\n",
    "    print(\"Data shape:\", data.shape)\n",
    "    print(\"Zeitraum:\", data[\"date\"].min(), \"bis\", data[\"date\"].max())\n",
    "else:\n",
    "    print(\"Keine Daten verarbeitet – Sammlung ist leer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d17a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53939"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Datenbank speichern\n",
    "# Erstellung SQL-Datenbank (dwh.sqlite3)\n",
    "connection = sqlite3.connect(SQL_PATH)\n",
    "\n",
    "# Wordcount-DataFrame in eine Tabelle erstellen\n",
    "data.to_sql(\"wordcount\", connection, index=False, if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc32eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL-Pfad: c:\\Users\\Marta\\Documents\\Marta\\DBU\\ADSC11 ADS-01\\Studienarbeit\\newspaper-scraping\\output\\dwh.sqlite3\n",
      "Existiert die Datei? True\n"
     ]
    }
   ],
   "source": [
    "print(\"SQL-Pfad:\", SQL_PATH)\n",
    "print(\"Existiert die Datei?\", os.path.exists(SQL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e35aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabellen: [('wordcount',)]\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(SQL_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Tabellen im DWH anzeigen\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(\"Tabellen:\", cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3763cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142, 'sz', 'sz', '2025-02-08')\n",
      "(121, 'plus', 'sz', '2025-02-08')\n",
      "(26, 'anzeigen', 'sz', '2025-02-08')\n",
      "(10, 'magazin', 'sz', '2025-02-08')\n",
      "(9, 'artikel', 'sz', '2025-02-08')\n"
     ]
    }
   ],
   "source": [
    "# Zeige 5 Zeilen aus der Tabelle 'wordcount'\n",
    "cursor.execute(\"SELECT * FROM wordcount LIMIT 5;\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newspaper-scraping-08G4y0J9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
