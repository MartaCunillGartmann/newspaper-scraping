{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63b0cd8-f07a-4d08-917e-e19168fe12d7",
   "metadata": {},
   "source": [
    "### Datenaufbereitung\n",
    "\n",
    "Dieses Notebook verarbeitet HTML- und CSV-Dateien mit Textdaten aus verschiedenen Quellen und bereitet sie für die Analyse von Wortfrequenzen und Medieninhalten auf. \n",
    "\n",
    "Die wichtigsten Schritte:\n",
    "- tar.gz-Archive entpacken und .html- sowie .csv-Dateien extrahieren  \n",
    "- HTML-Inhalte bereinigen, in Wörter aufspalten und Stoppwörter entfernen  \n",
    "- Wortfrequenzen je Medium und Datum zählen  \n",
    "- Ergebnisse in einer SQLite-Datenbank sowie als CSV-Dateien speichern  \n",
    "- Fehler beim Einlesen und Verarbeiten automatisch protokollieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c86a9f",
   "metadata": {},
   "source": [
    "##### 1. Import der benötigten Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da26eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import os # Dateipfaden\n",
    "import pandas as pd # Tabellenverarbeitung (DataFrames)\n",
    "import sys  # Systemfunktionen \n",
    "\n",
    "# Einlesen & entpacken Archivdateien\n",
    "from glob import glob # Mehrere Dateien suchen\n",
    "import tarfile # .tar.gz Dateien entpacken\n",
    "\n",
    "# Bearbeiten von html-Dateien\n",
    "from bs4 import BeautifulSoup  # HTML auslesen und bereinigen\n",
    "import requests # HTTP-Anfragen\n",
    "\n",
    "# Speicherung\n",
    "from datetime import datetime # Datumsangaben\n",
    "import sqlite3 # SQL-Datenbank\n",
    "\n",
    "# Eigene Funktionen (ausgelagert)\n",
    "sys.path.append(\"../scripts\") # Pfad zu den Funktionen\n",
    "# Funktionen aus datenaufbereitung.py \n",
    "from datenaufbereitung import (\n",
    "    load_stopwords,\n",
    "    read_html_file,\n",
    "    process_html\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5691b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der maximal sichtbaren Warnungen bei leerem Ergebnis (für saubere PDF-Darstellung)\n",
    "MAX_WARNUNGEN = 10\n",
    "warnungen_ausgegeben = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9966097-f518-4e15-a890-ce37773747dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade \n",
    "# Projektverzeichnis \n",
    "PROJECT_ROOT = r\"D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\"\n",
    "\n",
    "# Input-Pfade\n",
    "INPUT_PATH = os.path.join(PROJECT_ROOT, \"input\", \"raw\") # Rohdaten\n",
    "DATA_LAKE_PATH = os.path.join(INPUT_PATH, \"data-lake\") # HTML- und CSV-Dateien\n",
    "ZIP_PATH = os.path.join(INPUT_PATH, \"downloaded_zips\") # ZIP-Dateien\n",
    "\n",
    "# Output-Pfade\n",
    "OUTPUT_PATH = os.path.join(PROJECT_ROOT, \"output\") # Ergebnisse\n",
    "STORAGE_PATH = DATA_LAKE_PATH\n",
    "SQL_PATH = os.path.join(OUTPUT_PATH, \"dwh.sqlite3\") # SQLite-Datenbank\n",
    "CSV_PATH = os.path.join(OUTPUT_PATH, \"wordcount_news.csv\") # Exportierte CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14008b30",
   "metadata": {},
   "source": [
    "#### 2. Archiv entpacken und Dateien speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19942300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Mit tarfile Rohdaten (Format .tar.gz) einlesen, extrahieren & entpacken\n",
    "def extract_tar_file(ZIP_PATH, DATA_LAKE_PATH): \n",
    "    with tarfile.open(ZIP_PATH, \"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            # Zielpfad für jede Datei \n",
    "            filename = os.path.basename(member.name)\n",
    "            \n",
    "            # HTML- und CSV-Dateien extrahieren\n",
    "            if filename.endswith(\".html\") or filename.endswith(\".csv\"):\n",
    "                target_path = os.path.join(DATA_LAKE_PATH, filename)\n",
    "                \n",
    "                # Skippen, wenn Datei schon existiert\n",
    "                if os.path.exists(target_path):\n",
    "                    continue\n",
    "                \n",
    "                # Entpacken in HTML-Datei\n",
    "                with open(target_path, \"wb\") as f:\n",
    "                    f.write(tar.extractfile(member).read())\n",
    "                \n",
    "                print(f\"Entpackt: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc907a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entpackung: Mit glob nach .tar.gz-Dateien im ZIP-Ordner suchen und extrahieren\n",
    "for zip_path in glob(os.path.join(ZIP_PATH, \"*.tar.gz\")):\n",
    "    extract_tar_file(zip_path, DATA_LAKE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1e0bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl CSV-Dateien: 1490\n",
      "Anzahl HTML-Dateien: 83557\n"
     ]
    }
   ],
   "source": [
    "# Verfügbare HTML- und CSV-Dateien im DATA_LAKE_PATH zählen und zurückgeben\n",
    "html_files = glob(os.path.join(DATA_LAKE_PATH, \"*.html\"))\n",
    "csv_files = [f for f in glob(os.path.join(DATA_LAKE_PATH, \"*.csv\"))]\n",
    "\n",
    "print(\"Anzahl CSV-Dateien:\", len(csv_files))\n",
    "print(\"Anzahl HTML-Dateien:\", len(html_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d0fd0",
   "metadata": {},
   "source": [
    "#### 3. Funktionen zur Verarbeitung definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df080ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stoppwörter laden\n",
    "stopwords_list = load_stopwords()\n",
    "# Fehlerliste für HTML-Dateien\n",
    "failed_html = []\n",
    "\n",
    "# Funktion: Eine Artikelzeile (Metadaten und zugehörige HTML-Datei) verarbeiten und Wortfrequenzen ermitteln\n",
    "def process_newspaper(newspaper, failed_html):\n",
    "    # Zähler für Warnungen bei leeren Ergebnissen (für saubere PDF-Darstellung)\n",
    "    global warnungen_ausgegeben\n",
    "\n",
    "    # Dateipfad aus Metadaten ermitteln\n",
    "    filename = os.path.basename(newspaper[\"file_name\"])  \n",
    "    full_path = os.path.join(DATA_LAKE_PATH, filename)   \n",
    "\n",
    "    # Encoding aus Metadaten lesen\n",
    "    encoding = newspaper[\"encoding\"].lower()\n",
    "    \n",
    "    # HTML laden und in Wörter zerlegen (ausgelagerte Funktionen)\n",
    "    html = read_html_file(full_path, failed_html, encoding)\n",
    "    items = process_html(html, stopwords_list)\n",
    "\n",
    "    # Wortfrequenzen berechnen und als DataFrame speichern\n",
    "    count = pd.Series(items).value_counts()\n",
    "    count_df = count.to_frame()\n",
    "    count_df.columns = [\"count\"]\n",
    "    count_df[\"word\"] = count_df.index\n",
    "    count_df[\"source\"] = newspaper[\"name\"]\n",
    "    count_df[\"date\"] = newspaper[\"date\"]\n",
    "\n",
    "    # Warnung bei leeren Ergebnissen begrenzen (für saubere PDF-Darstellung)\n",
    "    if count_df.empty:\n",
    "        if warnungen_ausgegeben < MAX_WARNUNGEN:\n",
    "            print(f\"[WARNUNG] Leeres Ergebnis für: {filename} ({newspaper['name']})\")\n",
    "            warnungen_ausgegeben += 1\n",
    "        elif warnungen_ausgegeben == MAX_WARNUNGEN:\n",
    "            print(\"[INFO] Weitere leere Ergebnisse werden nicht mehr angezeigt.\")\n",
    "            warnungen_ausgegeben += 1\n",
    "\n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422a610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Einzelnen Artikelzeile verarbeiten für Fehlerprotokoll und Ergebnis speichern\n",
    "def process_wrapper(row, collection, failed_list):\n",
    "    try:\n",
    "        df = process_newspaper(row, failed_list)\n",
    "        if df is not None and not df.empty:\n",
    "            collection.append(df)\n",
    "    except Exception as e:\n",
    "        # Fehler bei der Artikelverarbeitung protokollieren und in failed_list speichern\n",
    "        print(f\"[ERROR] Fehler bei: {row.get('name', 'unbekannt')} – {e}\")\n",
    "        failed_list.append({\n",
    "            \"filename\": row.get(\"file_name\", \"unbekannt\"),\n",
    "            \"source\": row.get(\"name\", \"unbekannt\"),\n",
    "            \"error\": str(e)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47cbb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Alle Artikel eines Medien-Clusters verarbeiten und Wortfrequenzen als CSV speichern\n",
    "def verarbeite_cluster(cluster_name, zielmedien, output_csv, failed_list):\n",
    "    # Zwischenspeicher für Artikel\n",
    "    collection = []\n",
    "    dateien_verarbeitet = 0\n",
    "\n",
    "    # Vorherige Ergebnisdatei löschen, um Duplikate zu vermeiden\n",
    "    if os.path.exists(output_csv):\n",
    "        os.remove(output_csv)\n",
    "\n",
    "    # Alle CSV-Dateien mit Metadaten durchsuchen\n",
    "    for filepath in sorted(glob(os.path.join(STORAGE_PATH, \"*.csv\"))):\n",
    "        dateiname = os.path.basename(filepath)\n",
    "\n",
    "        # Datei überspringen, wenn sie leer ist\n",
    "        if os.path.getsize(filepath) == 0:\n",
    "            print(f\"[INFO] Übersprungen: Leere Datei {dateiname}\")\n",
    "            continue\n",
    "\n",
    "        # Datei einlesen \n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "        # Fehler: Datei hat keine Spalten protokollieren und in fail_list speichern\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"[WARNUNG] Datei hat keine Spalten: {dateiname}\")\n",
    "            failed_list.append({\n",
    "                \"filename\": dateiname,\n",
    "                \"source\": \"Metadaten-Datei\",\n",
    "                \"error\": \"EmptyDataError: Datei hat keine Spalten\"\n",
    "            })\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # Fehler: Datei kann nicht gelesen werden protokollieren und in fail_list speichern\n",
    "            print(f\"[FEHLER] Datei konnte nicht gelesen werden: {dateiname} :{e}\")\n",
    "            failed_list.append({\n",
    "                \"filename\": dateiname,\n",
    "                \"source\": \"Metadaten-Datei\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Filter: nur Medien aus dem aktuellen Cluster\n",
    "        df = df[df[\"name\"].isin(zielmedien)]\n",
    "\n",
    "        if not df.empty:\n",
    "            # Einzelne Artikelzeilen verarbeiten\n",
    "            df.apply(lambda row: process_wrapper(row, collection, failed_list), axis=1)\n",
    "\n",
    "            # Ergebnisse blockweise in CSV schreiben\n",
    "            for chunk in collection:\n",
    "                chunk.to_csv(output_csv, mode=\"a\", index=False, header=not os.path.exists(output_csv))\n",
    "\n",
    "            # Zwischenspeicher leeren\n",
    "            collection = []\n",
    "            # Zählen\n",
    "            dateien_verarbeitet += 1\n",
    "\n",
    "    if os.path.exists(output_csv):\n",
    "        # Verarbeitungsstatistik\n",
    "        df_result = pd.read_csv(output_csv)\n",
    "        print(f\"[INFO] Cluster {cluster_name}: {df_result.shape[0]} Einträge aus {dateien_verarbeitet} Dateien verarbeitet\")\n",
    "    else:\n",
    "        print(f\"[INFO] Cluster {cluster_name}: Keine Einträge gespeichert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e3e1d",
   "metadata": {},
   "source": [
    "#### 4. Iteratives Anwenden der Verarbeitungsfunktionen auf die Medien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7757ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter definieren \n",
    "# Zielmedien definieren\n",
    "zielmedien_or = {\"dlf\", \"tagesschau\"}  # Öffentlich-rechtlich\n",
    "zielmedien_wm = {\"handelsblatt\", \"wiwo\", \"mm\", \"boerse\"}  # Wirtschaftsmedien\n",
    "zielmedien_gm = {\"sz\", \"zeit\", \"faz\", \"taz\", \"welt\", \"spiegel\", \"stern\"}  # Große Medien\n",
    "zielmedien_rm = {\"abendblatt\", \"berliner\", \"tagesspiegel\"}  # Regionale Medien\n",
    "zielmedien_di = {\"ntv\", \"pioneer\", \"dw-de\"}  # digitale Nachrichtenportale\n",
    "zielmedien_tech = {\"heise\", \"golem\", \"netzpolitik\", \"t3n\"}  # Technologie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "121f5474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Übersprungen: Leere Datei 2021-06-03.csv\n",
      "[INFO] Cluster Öffentlich-rechtlich: 2934465 Einträge aus 1489 Dateien verarbeitet\n",
      "[INFO] Übersprungen: Leere Datei 2021-06-03.csv\n",
      "[INFO] Cluster Wirtschaftsmedien: 8346489 Einträge aus 1489 Dateien verarbeitet\n",
      "[WARNUNG] Leeres Ergebnis für: 2021-04-02-welt.html (welt)\n",
      "[WARNUNG] Leeres Ergebnis für: 2021-05-22-welt.html (welt)\n",
      "[WARNUNG] Leeres Ergebnis für: 2021-05-29-welt.html (welt)\n",
      "[INFO] Übersprungen: Leere Datei 2021-06-03.csv\n",
      "[WARNUNG] Leeres Ergebnis für: 2021-07-02-welt.html (welt)\n",
      "[WARNUNG] Leeres Ergebnis für: 2021-07-18-welt.html (welt)\n",
      "[WARNUNG] Leeres Ergebnis für: 2021-07-31-welt.html (welt)\n",
      "[WARNUNG] Leeres Ergebnis für: 2021-08-01-welt.html (welt)\n",
      "[WARNUNG] Leeres Ergebnis für: 2021-10-02-welt.html (welt)\n",
      "[WARNUNG] Leeres Ergebnis für: 2021-10-11-welt.html (welt)\n",
      "[WARNUNG] Leeres Ergebnis für: 2021-10-17-welt.html (welt)\n",
      "[INFO] Weitere leere Ergebnisse werden nicht mehr angezeigt.\n",
      "[INFO] Cluster Große Medien: 19988321 Einträge aus 1489 Dateien verarbeitet\n",
      "[INFO] Übersprungen: Leere Datei 2021-06-03.csv\n",
      "[INFO] Cluster Regionale Medien: 5998850 Einträge aus 1489 Dateien verarbeitet\n",
      "[INFO] Übersprungen: Leere Datei 2021-06-03.csv\n",
      "[INFO] Cluster Digitale Nachrichtsportale: 5368509 Einträge aus 1489 Dateien verarbeitet\n",
      "[INFO] Übersprungen: Leere Datei 2021-06-03.csv\n",
      "[INFO] Cluster Technologie: 5461011 Einträge aus 1489 Dateien verarbeitet\n"
     ]
    }
   ],
   "source": [
    "# Cluster einzeln verarbeiten\n",
    "# Fehlerliste\n",
    "failed_list = []\n",
    "\n",
    "# Cluster verarbeiten\n",
    "verarbeite_cluster(\"Öffentlich-rechtlich\", zielmedien_or, os.path.join(OUTPUT_PATH,\"cluster_oeffentlich.csv\"), failed_list)\n",
    "verarbeite_cluster(\"Wirtschaftsmedien\", zielmedien_wm, os.path.join(OUTPUT_PATH,\"cluster_wirtschaft.csv\"), failed_list)\n",
    "verarbeite_cluster(\"Große Medien\", zielmedien_gm, os.path.join(OUTPUT_PATH,\"cluster_grossemedien.csv\"), failed_list)\n",
    "verarbeite_cluster(\"Regionale Medien\", zielmedien_rm, os.path.join(OUTPUT_PATH,\"cluster_regiomedien.csv\"), failed_list)\n",
    "verarbeite_cluster(\"Digitale Nachrichtsportale\", zielmedien_di, os.path.join(OUTPUT_PATH,\"cluster_digital.csv\"), failed_list)\n",
    "verarbeite_cluster(\"Technologie\", zielmedien_tech, os.path.join(OUTPUT_PATH,\"cluster_tech.csv\"), failed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4b482",
   "metadata": {},
   "source": [
    "#### 5. Speicherung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0221917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisdateien laden, DataFrame pro Cluster\n",
    "df_or = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_oeffentlich.csv\"))\n",
    "df_wm = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_wirtschaft.csv\"))\n",
    "df_gm = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_grossemedien.csv\"))\n",
    "df_rm = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_regiomedien.csv\"))\n",
    "df_di = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_digital.csv\"))\n",
    "df_tech = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_tech.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5973c3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gesamt-Daten zusammengeführt\n",
      "[INFO] Zeilenanzahl: 48097645\n"
     ]
    }
   ],
   "source": [
    "# DataFrames aus Cluster-Dateien zusammenführen\n",
    "df_medien = pd.concat([df_or, df_wm, df_gm, df_rm, df_di, df_tech], axis=0, ignore_index=True)\n",
    "print(\"[INFO] Gesamt-Daten zusammengeführt\")\n",
    "print(\"[INFO] Zeilenanzahl:\", len(df_medien))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb265712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Keine Fehler beim Verarbeiten der Cluster\n"
     ]
    }
   ],
   "source": [
    "# Fehlerliste speichern\n",
    "if failed_list:\n",
    "    fehler_path = os.path.join(OUTPUT_PATH, \"html_failed.csv\")\n",
    "    pd.DataFrame(failed_list).to_csv(fehler_path, index=False)\n",
    "    print(f\"[INFO] Fehlerhafte HTML-/CSV-Dateien gespeichert: {fehler_path}\")\n",
    "else:\n",
    "    print(\"[INFO] Keine Fehler beim Verarbeiten der Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1fa0c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] In Datenbank gespeichert\n"
     ]
    }
   ],
   "source": [
    "# In SQLite-Datenbank speichern\n",
    "conn = sqlite3.connect(SQL_PATH)\n",
    "\n",
    "# Ergebnis blockweise in die SQLite-Datenbank schreiben\n",
    "df_medien.to_sql(\"wordcount\", conn, if_exists=\"replace\", index=False, chunksize=10_000)\n",
    "\n",
    "# Verbindung schließen\n",
    "conn.close()\n",
    "print(\"[INFO] In Datenbank gespeichert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "524205ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Als CSV gespeichert\n"
     ]
    }
   ],
   "source": [
    "# DataFrame als CSV speichern\n",
    "df_medien.to_csv(CSV_PATH, index=False)\n",
    "print(\"[INFO] Als CSV gespeichert\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
