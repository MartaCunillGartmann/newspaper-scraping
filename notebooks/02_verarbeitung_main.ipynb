{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63b0cd8-f07a-4d08-917e-e19168fe12d7",
   "metadata": {},
   "source": [
    "### Datenaufbereitung\n",
    "\n",
    "Dieses Notebook verarbeitet HTML- und CSV-Dateien mit Textdaten aus verschiedenen Quellen und bereitet sie für die Analyse von Wortfrequenzen und Medieninhalten auf. \n",
    "\n",
    "Die wichtigsten Schritte:\n",
    "- HTML-Inhalte bereinigen, in Wörter aufspalten und Stoppwörter entfernen  \n",
    "- Wortfrequenzen je Medium und Datum zählen  \n",
    "- Ergebnisse in einer SQLite-Datenbank sowie als CSV-Dateien speichern  \n",
    "- Fehler beim Einlesen und Verarbeiten protokollieren\n",
    "\n",
    "Dieses Skript setzt voraus, dass die Daten bereits entpackt und geprüft wurden (siehe: 01_entpacken_und_rohdatenanalyse.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c86a9f",
   "metadata": {},
   "source": [
    "##### 1. Import der benötigten Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da26eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import os # Dateipfaden\n",
    "import pandas as pd # Tabellenverarbeitung (DataFrames)\n",
    "from glob import glob # Mehrere Dateien suchen\n",
    "\n",
    "# Speicherung\n",
    "import sqlite3 # SQL-Datenbank\n",
    "\n",
    "# Eigene Funktionen (ausgelagert)\n",
    "import sys  # Systemfunktionen \n",
    "sys.path.append(\"..\") # Pfad zu .py Datei\n",
    "from scripts.datenaufbereitung import (load_stopwords, read_html_file, process_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9966097-f518-4e15-a890-ce37773747dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade \n",
    "# Projektverzeichnis \n",
    "PROJECT_ROOT = r\"D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\"\n",
    "\n",
    "# Input-Pfade\n",
    "INPUT_PATH = os.path.join(PROJECT_ROOT, \"input\", \"raw\") \n",
    "DATA_LAKE_PATH = os.path.join(INPUT_PATH, \"data-lake\") \n",
    "\n",
    "\n",
    "# Output-Pfade\n",
    "OUTPUT_PATH = os.path.join(PROJECT_ROOT, \"output\") \n",
    "STORAGE_PATH = DATA_LAKE_PATH\n",
    "SQL_PATH = os.path.join(OUTPUT_PATH, \"dwh.sqlite3\") \n",
    "CSV_PATH = os.path.join(OUTPUT_PATH, \"wordcount_news.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d0fd0",
   "metadata": {},
   "source": [
    "#### 2. Funktionen zur Verarbeitung definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0393bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter definieren\n",
    "# Zielmedien definieren\n",
    "zielmedien_or = {\"dlf\", \"tagesschau\"}  # Öffentlich-rechtlich\n",
    "zielmedien_wm = {\"handelsblatt\", \"wiwo\", \"mm\", \"boerse\"}  # Wirtschaftsmedien\n",
    "zielmedien_gm = {\"sz\", \"zeit\", \"faz\", \"taz\", \"welt\", \"spiegel\", \"stern\"}  # Große Medien\n",
    "zielmedien_rm = {\"abendblatt\", \"berliner\", \"tagesspiegel\"}  # Regionale Medien\n",
    "zielmedien_di = {\"ntv\", \"pioneer\", \"dw-de\"}  # digitale Nachrichtenportale\n",
    "zielmedien_tech = {\"heise\", \"golem\", \"netzpolitik\", \"t3n\"}  # Technologie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e45bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter definieren\n",
    "# Cluster-Zuordnung \n",
    "cluster_map = {\n",
    "    \"Öffentlich-rechtlich\": zielmedien_or,\n",
    "    \"Wirtschaft\": zielmedien_wm,\n",
    "    \"Große Medien\": zielmedien_gm,\n",
    "    \"Regional\": zielmedien_rm,\n",
    "    \"Digital\": zielmedien_di,\n",
    "    \"Technologie\": zielmedien_tech,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cac269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter definieren\n",
    "# Mapping: Medium zu Clustername\n",
    "medium_to_cluster = {}\n",
    "for cluster_name, medien_set in cluster_map.items():\n",
    "    for medium in medien_set:\n",
    "        medium_to_cluster[medium] = cluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29529cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stoppwörter laden\n",
    "stopwords_list = load_stopwords()\n",
    "# Fehlerliste für HTML-Dateien\n",
    "failed_html = []\n",
    "\n",
    "# Funktion: Eine Artikelzeile (Metadaten und zugehörige HTML-Datei) verarbeiten und Wortfrequenzen ermitteln\n",
    "def process_newspaper(newspaper, failed_html):\n",
    "    # Dateipfad aus Metadaten ermitteln\n",
    "    filename = os.path.basename(newspaper[\"file_name\"])  \n",
    "    full_path = os.path.join(DATA_LAKE_PATH, filename)   \n",
    "\n",
    "    # Encoding aus Metadaten lesen\n",
    "    encoding = newspaper[\"encoding\"].lower()\n",
    "    \n",
    "    # HTML laden und in Wörter zerlegen (ausgelagerte Funktionen)\n",
    "    html = read_html_file(full_path, failed_html, encoding)\n",
    "    items = process_html(html, stopwords_list)\n",
    "\n",
    "    # # Wortfrequenzen berechnen, Metadaten ergänzen (Quelle, Datum, Cluster) und als DataFrame speichern\n",
    "    count = pd.Series(items).value_counts()\n",
    "    count_df = count.to_frame()\n",
    "    count_df.columns = [\"count\"]\n",
    "    count_df[\"word\"] = count_df.index\n",
    "    count_df[\"source\"] = newspaper[\"name\"]\n",
    "    count_df[\"date\"] = newspaper[\"date\"]\n",
    "    count_df[\"cluster\"] = medium_to_cluster[newspaper[\"name\"]]\n",
    "\n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422a610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Einzelnen Artikelzeile verarbeiten für Fehlerprotokoll und Ergebnis speichern\n",
    "def process_wrapper(row, collection, failed_list):\n",
    "    try:\n",
    "        df = process_newspaper(row, failed_list)\n",
    "        if df is not None and not df.empty:\n",
    "            collection.append(df)\n",
    "    except Exception as e:\n",
    "        # Fehler bei der Artikelverarbeitung protokollieren und in failed_list speichern\n",
    "        print(f\"[ERROR] Fehler bei: {row.get('name', 'unbekannt')} – {e}\")\n",
    "        failed_list.append({\n",
    "            \"filename\": row.get(\"file_name\", \"unbekannt\"),\n",
    "            \"source\": row.get(\"name\", \"unbekannt\"),\n",
    "            \"error\": str(e)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47cbb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Alle Artikel eines Medien-Clusters verarbeiten und Wortfrequenzen als CSV speichern\n",
    "def verarbeite_cluster(cluster_name, zielmedien, output_csv, failed_list):\n",
    "    # Zwischenspeicher für Artikel\n",
    "    collection = []\n",
    "    dateien_verarbeitet = 0\n",
    "\n",
    "    # Vorherige Ergebnisdatei löschen, um Duplikate zu vermeiden\n",
    "    if os.path.exists(output_csv):\n",
    "        os.remove(output_csv)\n",
    "\n",
    "    # Alle CSV-Dateien mit Metadaten durchsuchen\n",
    "    for filepath in sorted(glob(os.path.join(STORAGE_PATH, \"*.csv\"))):\n",
    "        dateiname = os.path.basename(filepath)\n",
    "\n",
    "        # Datei überspringen, wenn sie leer ist\n",
    "        if os.path.getsize(filepath) == 0:\n",
    "            print(f\"[INFO] Übersprungen: Leere Datei {dateiname}\")\n",
    "            continue\n",
    "\n",
    "        # Datei einlesen \n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "        # Fehler: Datei hat keine Spalten protokollieren und in fail_list speichern\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"[WARNING] Datei hat keine Spalten: {dateiname}\")\n",
    "            failed_list.append({\n",
    "                \"filename\": dateiname,\n",
    "                \"source\": \"Metadaten-Datei\",\n",
    "                \"error\": \"EmptyDataError: Datei hat keine Spalten\"\n",
    "            })\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            # Fehler: Datei kann nicht gelesen werden protokollieren und in fail_list speichern\n",
    "            print(f\"[FEHLER] Datei konnte nicht gelesen werden: {dateiname} :{e}\")\n",
    "            failed_list.append({\n",
    "                \"filename\": dateiname,\n",
    "                \"source\": \"Metadaten-Datei\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Filter: nur Medien aus dem aktuellen Cluster\n",
    "        df = df[df[\"name\"].isin(zielmedien)]\n",
    "\n",
    "        if not df.empty:\n",
    "            # Einzelne Artikelzeilen verarbeiten\n",
    "            df.apply(lambda row: process_wrapper(row, collection, failed_list), axis=1)\n",
    "\n",
    "            # Ergebnisse blockweise in CSV schreiben\n",
    "            for chunk in collection:\n",
    "                chunk.to_csv(output_csv, mode=\"a\", index=False, header=not os.path.exists(output_csv))\n",
    "\n",
    "            # Zwischenspeicher leeren\n",
    "            collection = []\n",
    "            # Zählen\n",
    "            dateien_verarbeitet += 1\n",
    "\n",
    "    if os.path.exists(output_csv):\n",
    "        # Verarbeitungsstatistik\n",
    "        df_result = pd.read_csv(output_csv)\n",
    "        print(f\"[INFO] Cluster {cluster_name}: {df_result.shape[0]} Einträge aus {dateien_verarbeitet} Dateien verarbeitet\")\n",
    "    else:\n",
    "        print(f\"[INFO] Cluster {cluster_name}: Keine Einträge gespeichert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e3e1d",
   "metadata": {},
   "source": [
    "#### 3. Iteratives Anwenden der Verarbeitungsfunktionen auf die Medien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121f5474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cluster Öffentlich-rechtlich: 2936012 Einträge aus 1490 Dateien verarbeitet\n",
      "[INFO] Cluster Wirtschaftsmedien: 8352533 Einträge aus 1490 Dateien verarbeitet\n",
      "[INFO] Cluster Große Medien: 20001815 Einträge aus 1490 Dateien verarbeitet\n",
      "[INFO] Cluster Regionale Medien: 6002528 Einträge aus 1490 Dateien verarbeitet\n",
      "[INFO] Cluster Digitale Nachrichtsportale: 5371784 Einträge aus 1490 Dateien verarbeitet\n",
      "[INFO] Cluster Technologie: 5464468 Einträge aus 1490 Dateien verarbeitet\n"
     ]
    }
   ],
   "source": [
    "# Cluster einzeln verarbeiten\n",
    "# Fehlerliste\n",
    "failed_list = []\n",
    "\n",
    "# Cluster verarbeiten\n",
    "verarbeite_cluster(\"Öffentlich-rechtlich\", zielmedien_or, os.path.join(OUTPUT_PATH,\"cluster_oeffentlich.csv\"), failed_list)\n",
    "verarbeite_cluster(\"Wirtschaftsmedien\", zielmedien_wm, os.path.join(OUTPUT_PATH,\"cluster_wirtschaft.csv\"), failed_list)\n",
    "verarbeite_cluster(\"Große Medien\", zielmedien_gm, os.path.join(OUTPUT_PATH,\"cluster_grossemedien.csv\"), failed_list)\n",
    "verarbeite_cluster(\"Regionale Medien\", zielmedien_rm, os.path.join(OUTPUT_PATH,\"cluster_regiomedien.csv\"), failed_list)\n",
    "verarbeite_cluster(\"Digitale Nachrichtsportale\", zielmedien_di, os.path.join(OUTPUT_PATH,\"cluster_digital.csv\"), failed_list)\n",
    "verarbeite_cluster(\"Technologie\", zielmedien_tech, os.path.join(OUTPUT_PATH,\"cluster_tech.csv\"), failed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe4b482",
   "metadata": {},
   "source": [
    "#### 4. Speicherung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0221917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisdateien laden, DataFrame pro Cluster\n",
    "df_or = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_oeffentlich.csv\"))\n",
    "df_wm = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_wirtschaft.csv\"))\n",
    "df_gm = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_grossemedien.csv\"))\n",
    "df_rm = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_regiomedien.csv\"))\n",
    "df_di = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_digital.csv\"))\n",
    "df_tech = pd.read_csv(os.path.join(OUTPUT_PATH, \"cluster_tech.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5973c3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Gesamt-Daten zusammengeführt\n",
      "[INFO] Zeilenanzahl: 48129140\n"
     ]
    }
   ],
   "source": [
    "# DataFrames aus Cluster-Dateien zusammenführen\n",
    "df_medien = pd.concat([df_or, df_wm, df_gm, df_rm, df_di, df_tech], axis=0, ignore_index=True)\n",
    "print(\"[INFO] Gesamt-Daten zusammengeführt\")\n",
    "print(\"[INFO] Zeilenanzahl:\", len(df_medien))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb265712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Keine Fehler beim Verarbeiten der Cluster\n"
     ]
    }
   ],
   "source": [
    "# Fehlerliste als DataFrame umwandeln und als CSV exportieren\n",
    "if failed_list:\n",
    "    fehler_path = os.path.join(OUTPUT_PATH, \"html_failed.csv\")\n",
    "    pd.DataFrame(failed_list).to_csv(fehler_path, index=False)\n",
    "    print(f\"[INFO] Fehlerhafte HTML-/CSV-Dateien gespeichert: {fehler_path}\")\n",
    "else:\n",
    "    print(\"[INFO] Keine Fehler beim Verarbeiten der Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1fa0c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] In Datenbank gespeichert\n"
     ]
    }
   ],
   "source": [
    "# In SQLite-Datenbank speichern\n",
    "conn = sqlite3.connect(SQL_PATH)\n",
    "\n",
    "# Ergebnis blockweise in die SQLite-Datenbank schreiben\n",
    "df_medien.to_sql(\"wordcount\", conn, if_exists=\"replace\", index=False, chunksize=10_000)\n",
    "\n",
    "# Verbindung schließen\n",
    "conn.close()\n",
    "print(\"[INFO] In Datenbank gespeichert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "524205ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Als CSV gespeichert\n"
     ]
    }
   ],
   "source": [
    "# Als CSV speichern\n",
    "df_medien.to_csv(CSV_PATH, index=False)\n",
    "print(\"[INFO] Als CSV gespeichert\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
