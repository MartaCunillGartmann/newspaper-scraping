{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "421e4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Beispielpfad ‚Äì anpassen auf eine echte Datei\n",
    "file_path = \"D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping/input/raw/data-lake/2021-04-01-dlf.html\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74e2e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Artikel gezielt extrahieren\n",
    "def extract_text_by_medium(html, medium):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    if medium == \"tagesschau\":\n",
    "        text_tags = soup.find_all(\"p\", class_=True)\n",
    "    elif medium == \"deutschlandfunk\":\n",
    "        text_tags = soup.find_all(\"p\", class_=True)\n",
    "    elif medium == \"handelsblatt\":\n",
    "        text_tags = soup.find_all([\"h2\", \"h3\"])\n",
    "    elif medium == \"tagesspiegel\":\n",
    "        text_tags = soup.find_all(\"p\")\n",
    "    else:\n",
    "        return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return \" \".join(t.get_text(separator=\" \", strip=True) for t in text_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18ec1398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] L√§nge des extrahierten Texts: 0\n",
      "[INFO] Beispieltext: \n",
      "[INFO] Enth√§lt 'bvg'? ‚Üí False\n"
     ]
    }
   ],
   "source": [
    "# Medium anpassen!\n",
    "medium = \"deutschlandfunk\"\n",
    "\n",
    "text = extract_text_by_medium(html, medium)\n",
    "\n",
    "print(\"[INFO] L√§nge des extrahierten Texts:\", len(text))\n",
    "print(\"[INFO] Beispieltext:\", text[:300])\n",
    "print(\"[INFO] Enth√§lt 'bvg'? ‚Üí\", \"bvg\" in text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ae39839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û§ INTERVIEWNordrhein-Westfalens Gesundheitsminister Karl-Josef Laumann (CDU) hat die Ausnahmen von der\n",
      "‚û§ Im Mittelpunkt steht die Entscheidung, dass der Corona-Impfstoff von Astrazeneca in der Regel nur no\n",
      "‚û§ Das RKI meldet rund 24.000 Neuinfektionen und eine leicht gestiegene Inzidenz. Der Anteil der zuerst\n",
      "‚û§ Was die Neuinfektionen f√ºr die kommenden Wochen bedeuten\n",
      "‚û§ INTERVIEWIn der Diskussion √ºber die umstrittene Ostsee-Gaspipeline Nord Stream 2 hat Oliver Krischer\n",
      "‚û§ KOMMENTARDie Entscheidung Astrazeneca-Impfungen f√ºr die unter 60-J√§hrigen auszusetzen, sei richtig g\n",
      "‚û§ \"Wir sind gewohnt, dass wir unsere Impfstrategie immer wieder anpassen m√ºssen\"\n",
      "‚û§ Bei bestimmten Altersgruppen auf anderen Impfstoff setzen\n",
      "‚û§ Die wichtigsten Fragen und Antworten zur Astrazeneca-Entscheidung\n",
      "‚û§ INTERVIEWDas Coronavirus stammt sehr wahrscheinlich von Flederm√§usen und wurde durch einen Zwischenw\n",
      "‚û§ Mit dem Corona-Wiederaufbaufonds stehen 750 Milliarden Euro f√ºr den wirtschaftlichen Wiederaufbau be\n",
      "‚û§ Der Schiffsverkehr im Suezkanal flie√üt wieder. Doch die tagelange Blockade dieses Nadel√∂hrs des Welt\n",
      "‚û§ INTERVIEWRegisseur Raymond Ley hat den Wirecard-Skandal als Doku-Thriller verfilmt. Der Fall ist nic\n",
      "‚û§ In Corona-Zeiten entdecken viele Menschen wieder ihre Liebe zum Kochen. Auch das Konservieren von Le\n",
      "‚û§ +++ RKI: Anteil der Virus-Variante B.1.1.7 in Deutschland bei 88 Prozent +++\n",
      "‚û§ Impfchaos und Bund-L√§nder-Streit: Interview NRW-Gesundheitsminister Laumann, CDU\n",
      "‚û§ Streit um Nordstream 2: Interview mit Oliver Krischer, MdB Gr√ºne, Fraktionsvize\n",
      "‚û§ Tausche Avocado-Farm gegen etruskische Ruinen: Cornelia Funke zieht nach Italien\n",
      "‚û§ Treffen der Rechtspopulisten: Orban, Salvini und Morawiecki in Budapest\n",
      "‚û§ INTERVIEWMunter, angriffslustig, voller Energie ‚Äì so wirke CDU-Chef Armin Laschet, sagte der Politol\n",
      "‚û§ KOMMENTAREs h√§tte keiner Bund-L√§nder-Fehlentscheidung √ºber die Osterruhe bedurft, um zu verdeutliche\n",
      "‚û§ Der Tarifkonflikt in der Metall- und Elektroindustrie ist gel√∂st, in Nordrhein-Westfalen fanden Arbe\n",
      "‚û§ INTERVIEWBDA-Hauptgesch√§ftsf√ºhrer Steffen Kampeter sprach sich gegen Produktionseinschr√§nkungen im R\n",
      "‚û§ F√ºr die berauschende Wirkung des Alkohols sind unter anderem Abbauprodukte verantwortlich, die vor a\n",
      "‚û§ 23. Juni 2016: Die Briten entscheiden in einer Volksabstimmung zwischen \"Leave\" oder \"Remain\"; zwisc\n",
      "‚û§ Der deutsche Hochschullehrer Klaus Kinzler steht in Frankreich seit Wochen unter Polizeischutz. Ausl\n",
      "‚û§ Wie wichtig ein harter Lockdown im Kampf gegen steigende Infektionszahlen ist, zeigt der Blick nach \n",
      "‚û§ Machtmissbrauch, sexuelle √úbergriffe oder ein Klima der Angst - so lauten nicht selten Vorw√ºrfe, wen\n",
      "‚û§ Das Guggenheim Museum in Bilbao oder das Getty Center in Los Angeles: Die internationale Museums-Arc\n",
      "‚û§ Es ist wahrscheinlich das zurzeit ber√ºhmteste Gedicht der Welt: \"The Hill We Climb\" der US-amerikani\n",
      "‚û§ Von Hajka zu Hajka, von einer Hetzjagd der Deutschen zur n√§chsten, treibt es die Partisanengruppen i\n",
      "‚û§ Im Jahr 2012 hat das Land Hamburg Staatsvertr√§ge mit den muslimischen Verb√§nden im Land unterzeichne\n",
      "‚û§ W√§hrend des Vietnamkriegs st√ºrmte im M√§rz 1968 eine Einheit von US-Soldaten unter Leutnant William C\n",
      "‚û§ Trotz Corona-Pandemie sollen im Sommer in Tokio die Olympischen- und Paralympischen Spiele stattfind\n",
      "‚û§ Urspr√ºnglich wollte das UEFA-Exekutivkomitee bei seiner Sitzung am Mittwoch √ºber die umstrittene Ref\n"
     ]
    }
   ],
   "source": [
    "# Zeige mal alle <p>-Elemente mit Inhalt\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "for tag in soup.find_all(\"p\"):\n",
    "    text = tag.get_text(strip=True)\n",
    "    if len(text) > 50:\n",
    "        print(\"‚û§\", text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3739fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_file(filepath, encoding=\"utf-8\"):\n",
    "    \"\"\"Liest eine HTML-Datei als Textstring\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Pfad zur HTML-Datei\n",
    "        encoding (str): Zeichenkodierung (Standard: 'utf-8')\n",
    "\n",
    "    Returns:\n",
    "        str: Inhalt der HTML-Datei als Text\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=encoding) as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8e29af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = read_html_file(\"D:/DBU/ADSC11 ADS-01\\Studienarbeit/newspaper-scraping/input/raw/data-lake/2025-02-28-handelsblatt.html\", encoding=\"iso-8859-1\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Alle h2 anzeigen\n",
    "for h in soup.find_all(\"h2\"):\n",
    "    print(h.get_text(strip=True))\n",
    "\n",
    "# Oder: Alle p mit spezieller Klasse\n",
    "for p in soup.find_all(\"p\", class_=\"teaser-text\"):\n",
    "    print(p.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43d5ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç Medium: ZEIT\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: FAZ\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: BERLINER\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: HANDELSBLATT\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: NTV\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: SPIEGEL\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: STERN\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: TAGESSCHAU\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: WIWO\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marta\\AppData\\Local\\Temp\\ipykernel_21364\\4046225662.py:20: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
      "\n",
      "    filehandle = open(your filename)\n",
      "\n",
      "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Alle Medien, die du testen willst\n",
    "alle_medien = {\"berliner\", \"tagesschau\", \"handelsblatt\", \"stern\", \"wiwo\", \"faz\", \"spiegel\", \"ntv\", \"zeit\"}\n",
    "\n",
    "# Lade eine CSV-Datei mit mehreren Medien\n",
    "df = pd.read_csv(\"D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping/input/raw/data-lake/2025-02-25.csv\")  # Beispiel-Datei\n",
    "\n",
    "# Optional: Filter auf nur ausgew√§hlte Medien\n",
    "df = df[df[\"name\"].isin(alle_medien)]\n",
    "\n",
    "# F√ºr jedes Medium nur einen Artikel testen\n",
    "df_test = df.groupby(\"name\").head(1)\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    name = row[\"name\"]\n",
    "    html = row[\"file_name\"]  # falls in CSV enthalten\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    print(f\"\\nüìç Medium: {name.upper()}\")\n",
    "\n",
    "    p_tags = soup.find_all(\"p\")\n",
    "    count = 0\n",
    "    for tag in p_tags:\n",
    "        text = tag.get_text(strip=True)\n",
    "        if len(text) > 50:\n",
    "            print(\"‚û§\", text[:100])\n",
    "            count += 1\n",
    "        if count >= 3:\n",
    "            break\n",
    "    if count == 0:\n",
    "        print(\"‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d0fe2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DBU\\ADSC11 ADS-01\\Studienarbeit\\newspaper-scraping\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# Projektverzeichnis (z.‚ÄØB. .../notebooks)\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "print(PROJECT_ROOT)\n",
    "\n",
    "# Input-Pfade\n",
    "INPUT_PATH = os.path.join(PROJECT_ROOT, \"..\", \"input\", \"raw\")\n",
    "DATA_LAKE_PATH = os.path.join(INPUT_PATH, \"data-lake\")         # HTML- und CSV-Dateien\n",
    "ZIP_PATH = os.path.join(INPUT_PATH, \"downloaded_zips\")         # ZIP-Dateien\n",
    "\n",
    "# Output-Pfade\n",
    "OUTPUT_PATH = os.path.join(PROJECT_ROOT, \"..\", \"output\")\n",
    "STORAGE_PATH = os.path.join(\"..\", \"input\", \"raw\", \"data-lake\")\n",
    "SQL_PATH = os.path.join(OUTPUT_PATH, \"dwh.sqlite3\")               # SQLite-Datenbank\n",
    "CSV_PATH = os.path.join(OUTPUT_PATH, \"wordcount_news.csv\")        # Exportierte CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f6df666",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df_test.iterrows():\n",
    "    name = row[\"name\"]\n",
    "    encoding = row.get(\"encoding\", \"utf-8\")  # <- HIER definierst du 'encoding'\n",
    "    file_name = row[\"file_name\"]\n",
    "    file_path = os.path.join(DATA_LAKE_PATH, os.path.basename(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce139531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Versuche zu laden: d:\\DBU\\ADSC11 ADS-01\\Studienarbeit\\newspaper-scraping\\notebooks\\..\\input\\raw\\data-lake\\2025-02-25-wiwo.html mit Encoding ISO-8859-1\n",
      "üìÇ Existiert Datei? True\n"
     ]
    }
   ],
   "source": [
    "print(f\"üîç Versuche zu laden: {file_path} mit Encoding {encoding}\")\n",
    "print(\"üìÇ Existiert Datei?\", os.path.exists(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "773266e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç Medium: ZEIT\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: FAZ\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: BERLINER\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: HANDELSBLATT\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: NTV\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: SPIEGEL\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: STERN\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: TAGESSCHAU\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n",
      "\n",
      "üìç Medium: WIWO\n",
      "‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marta\\AppData\\Local\\Temp\\ipykernel_21364\\4046225662.py:20: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a filename than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the contents of a file on disk, then something has gone wrong. You should open the file first, using code like this:\n",
      "\n",
      "    filehandle = open(your filename)\n",
      "\n",
      "You can then feed the open filehandle into Beautiful Soup instead of using the filename.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a filename, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Alle Medien, die du testen willst\n",
    "alle_medien = {\"berliner\", \"tagesschau\", \"handelsblatt\", \"stern\", \"wiwo\", \"faz\", \"spiegel\", \"ntv\", \"zeit\"}\n",
    "\n",
    "# Lade eine CSV-Datei mit mehreren Medien\n",
    "df = pd.read_csv(\"D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping/input/raw/data-lake/2025-02-25.csv\")  # Beispiel-Datei\n",
    "\n",
    "# Optional: Filter auf nur ausgew√§hlte Medien\n",
    "df = df[df[\"name\"].isin(alle_medien)]\n",
    "\n",
    "# F√ºr jedes Medium nur einen Artikel testen\n",
    "df_test = df.groupby(\"name\").head(1)\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    name = row[\"name\"]\n",
    "    html = row[\"file_name\"]  # falls in CSV enthalten\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    print(f\"\\nüìç Medium: {name.upper()}\")\n",
    "\n",
    "    p_tags = soup.find_all(\"p\")\n",
    "    count = 0\n",
    "    for tag in p_tags:\n",
    "        text = tag.get_text(strip=True)\n",
    "        if len(text) > 50:\n",
    "            print(\"‚û§\", text[:100])\n",
    "            count += 1\n",
    "        if count >= 3:\n",
    "            break\n",
    "    if count == 0:\n",
    "        print(\"‚ö†Ô∏è Keine sinnvollen <p>-Inhalte gefunden.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2403c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_file(filepath, encoding=\"utf-8\"):\n",
    "    print(\"üß™ √ñffne:\", filepath)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"‚ùå Datei existiert nicht!\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=encoding, errors=\"ignore\") as f:\n",
    "            content = f.read()\n",
    "            print(\"‚úÖ Datei erfolgreich gelesen\")\n",
    "            return content\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler beim Lesen der Datei: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c483cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ √ñffne: d:\\DBU\\ADSC11 ADS-01\\Studienarbeit\\newspaper-scraping\\notebooks\\..\\input\\raw\\data-lake\\2025-02-25-zeit.html\n",
      "‚úÖ Datei erfolgreich gelesen\n",
      "üìÑ Inhalt beginnt mit: <!DOCTYPE html>\n",
      "<html lang=\"de\" class=\"no-js \" data-rebrush-23    >\n",
      "<head>\n",
      "    <meta charset=\"utf-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no, viewport-fit=cover\" id=\"viewport-meta\">\n",
      "    <title>ZEIT ONLINE | Nachrichten, News, Hintergr√ºnde und Debatte\n",
      "üìä Anzahl <p>-Tags: 164\n",
      "‚û§ Danke, dass Sie ZEIT ONLINE nutzen.\n",
      "‚û§ \n",
      "‚û§ Melden Sie sich jetzt mit Ihrem bestehenden Account an oder testen Sie unser digitales Abo mit Zugang zu allen Artikeln.\n"
     ]
    }
   ],
   "source": [
    "row = df_test.iloc[0]  # Nur einen Artikel testen\n",
    "name = row[\"name\"]\n",
    "file_name = row[\"file_name\"]\n",
    "encoding = row.get(\"encoding\", \"utf-8\")\n",
    "\n",
    "file_path = os.path.join(DATA_LAKE_PATH, os.path.basename(file_name))\n",
    "html = read_html_file(file_path, encoding=encoding)\n",
    "\n",
    "print(\"üìÑ Inhalt beginnt mit:\", html[:300] if html else \"Kein Inhalt\")\n",
    "\n",
    "if html:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    p_tags = soup.find_all(\"p\")\n",
    "    print(f\"üìä Anzahl <p>-Tags: {len(p_tags)}\")\n",
    "    for p in p_tags[:3]:\n",
    "        print(\"‚û§\", p.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "651bbe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Durchsuche <script>-Bl√∂cke nach Textinhalten...\n",
      "\n",
      "\n",
      "üìÑ Script #1 mit m√∂glichem Inhalt:\n",
      "\n",
      "        {\n",
      "            \"assetHost\": \"https://static.zeit.de/p/zeit.web\",\n",
      "            \"publicPath\": \"https://static.zeit.de/p/zeit.web/js/\",\n",
      "            \"jsconfHost\": \"https://static.zeit.de/static/js\",\n",
      "            \"cookieFallbackDomain\": \"zeit.de\",\n",
      "            \"profileURL\": \"https://profile.zeit.de\",\n",
      "            \"videoPlayers\": {\n",
      "                \"cp\": {\n",
      "                    \"withAds\": \"65fa926a-0fe0-4031-8cbf-9db35cecf64a\",\n",
      "                    \"withoutAds\": \"r1xb937iwZ\"\n",
      "                },\n",
      "                \"article\": {\n",
      "                    \"withAds\": \"c09a3b98-8829-47a5-b93b-c3cca8a4b5e9\",\n",
      "                    \"withoutAds\": \"NykzeyfYg\"\n",
      "                }\n",
      "            },\n",
      "            \"actualHost\": \"https://www.zeit.de\",\n",
      "            \"toggles\": {\n",
      "                \"abtesting\": true,\n",
      "                \"abtesting_progressbar\": false,\n",
      "                \"adblocker_user_analytics\": true,\n",
      "                \"autoplay_carousel_detects_swipes\": true,\n",
      "                \"autoplay_headed_zwe_area\": true,\n",
      "             \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# HTML wurde ja schon gelesen\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "print(\"\\nüîé Durchsuche <script>-Bl√∂cke nach Textinhalten...\\n\")\n",
    "\n",
    "for i, script in enumerate(soup.find_all(\"script\")):\n",
    "    content = script.string\n",
    "    if content and any(keyword in content.lower() for keyword in [\"bvg\", \"article\", \"body\", \"text\"]):\n",
    "        print(f\"\\nüìÑ Script #{i} mit m√∂glichem Inhalt:\")\n",
    "        print(content[:1000])  # Nur die ersten 1000 Zeichen anzeigen\n",
    "        break\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Keine relevanten <script>-Inhalte gefunden.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
