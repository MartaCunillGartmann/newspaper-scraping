{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "716d66f0",
   "metadata": {},
   "source": [
    "### Datenanreicherung – Artikellinks extrahieren\n",
    "\n",
    "Dieses Notebook extrahiert Headlines und Links aus den zur Verfügung gestellten HTML-Dateien ausgewählter Medien, die eines der definierten Keywords BVG, HVV (inkl. VHH, Hochbahn)oder MVG enthalten.\n",
    "\n",
    "Die wichtigsten Schritte:\n",
    "- Extraktion von Links und Headlines aus HTML-Dateien auf Basis der vorliegenden Metadaten\n",
    "- Speicherung der Ergebnisse als CSV-Dateien\n",
    "- Protokollierung potenzieller Verarbeitungsfehler zur Nachvollziehbarkeit\n",
    "\n",
    "Dieses Skript setzt voraus, dass die Daten bereits entpackt und geprüft wurden (siehe: 01_entpacken_und_rohdatenanalyse.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e9174",
   "metadata": {},
   "source": [
    "#### 1. Import benötigte Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aff0d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import os # Dateipfaden\n",
    "import pandas as pd # Tabellenverarbeitung (DataFrames)\n",
    "\n",
    "# HTML-Verarbeitung \n",
    "from bs4 import BeautifulSoup # HTML analysieren\n",
    "from glob import glob # Dateisuche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3c3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade\n",
    "# Projektverzeichnis\n",
    "PROJECT_ROOT = r\"D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\"\n",
    "\n",
    "# Input-Pfade\n",
    "INPUT_PATH = os.path.join(PROJECT_ROOT, \"input\", \"raw\") \n",
    "DATA_LAKE_PATH = os.path.join(INPUT_PATH, \"data-lake\") \n",
    "STORAGE_PATH = DATA_LAKE_PATH\n",
    "\n",
    "# Output-Pfad\n",
    "OUTPUT_PATH = os.path.join(PROJECT_ROOT, \"output\") \n",
    "CSV_PATH = os.path.join(OUTPUT_PATH, \"artikel_links_headlines.csv\")\n",
    "FAILED_PATH = os.path.join(OUTPUT_PATH, \"artikel_links_failed.csv\")\n",
    "LOG_PATH = os.path.join(OUTPUT_PATH, \"artikel_links_log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c37e3d",
   "metadata": {},
   "source": [
    "#### 2. Funktionen zur Verarbeitung definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3346cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter definieren\n",
    "# Auswahl an Medien\n",
    "MEDIEN = [\n",
    "    \"dlf\", \"tagesschau\",\n",
    "    \"handelsblatt\", \"wiwo\", \"mm\", \"boerse\",\n",
    "    \"sz\", \"zeit\", \"faz\", \"taz\", \"welt\", \"spiegel\", \"stern\",\n",
    "    \"abendblatt\", \"berliner\", \"tagesspiegel\",\n",
    "    \"ntv\", \"pioneer\", \"dw-de\",\n",
    "    \"heise\", \"golem\", \"netzpolitik\", \"t3n\"\n",
    "]\n",
    "\n",
    "# Keywords: Filterung von Artikellinks\n",
    "KEYWORDS = [\"bvg\", \"mvg\", \"hvv\", \"vhh\", \"hochbahn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "159f0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Links und Headlines aus HTML-Datei anhand der Metadatenzeile extrahieren\n",
    "def extract_links_and_headlines(newspaper):\n",
    "  \n",
    "    # Dateiname, Pfad, Encoding und Basis-URL aus den Metadaten\n",
    "    filename = os.path.basename(newspaper[\"file_name\"])\n",
    "    full_path = os.path.join(DATA_LAKE_PATH, filename)\n",
    "    encoding = newspaper[\"encoding\"].lower()\n",
    "    url_base = newspaper[\"final_url\"].strip()  \n",
    "\n",
    "    # HTML-Datei einlesen\n",
    "    try:\n",
    "        with open(full_path, \"r\", encoding=encoding) as f:\n",
    "            html = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Fehler beim Lesen von {filename}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # HTML parsen mit BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Ergebnisliste vorbereiten\n",
    "    records = []\n",
    "\n",
    "    # Link-Tags im HTML durchsuchen\n",
    "    for link_tag in soup.find_all(\"a\", href=True):\n",
    "        href = link_tag[\"href\"]\n",
    "        # Sichtbarer Text innerhalb des Links holen\n",
    "        text = link_tag.get_text(strip=True)\n",
    "        # Beide Texte für spätere Suche Keywords kombinieren\n",
    "        combined = (href + \" \" + text).lower()\n",
    "\n",
    "        # Weiterverarbeiten nur, wenn mindestens ein Keyword vorkommt\n",
    "        if any(kw in combined for kw in KEYWORDS):\n",
    "            # Vollständige URL rekonstruieren\n",
    "            if href.startswith(\"/\"):\n",
    "                full_url = url_base.rstrip(\"/\") + href\n",
    "            elif href.startswith(\"http\"):\n",
    "                full_url = href\n",
    "            else:\n",
    "                continue  \n",
    "\n",
    "            # Nur aufnehmen, wenn headline existiert\n",
    "            if text:\n",
    "                records.append({\n",
    "                    \"filename\": filename, # Dateilname\n",
    "                    \"source\": newspaper[\"name\"], # Medium\n",
    "                    \"date\": newspaper[\"date\"], # Veröffentlichungsdatum\n",
    "                    \"url\": full_url, # http-Adresse\n",
    "                    \"headline\": text # Headline\n",
    "                })\n",
    "\n",
    "    # Ergebnis als DataFrame speichern\n",
    "    return pd.DataFrame(records) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fee91e",
   "metadata": {},
   "source": [
    "#### 3. Iteratives Anwenden der Verarbeitungsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e2fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links und Headlines aus HTML-Datei anhand der Metadatenzeile extrahieren\n",
    "\n",
    "# Ergebnis-Listen\n",
    "results = []\n",
    "log_list = []\n",
    "failing_list = []\n",
    "\n",
    "# Alle Metadaten-CSV-Dateien durchlaufen\n",
    "csv_files = sorted(glob(os.path.join(STORAGE_PATH, \"*.csv\")))\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    try:        \n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Filter nur ausgewählte Medien\n",
    "        df = df[df[\"name\"].isin(MEDIEN)]\n",
    "\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # Für jede Zeile (HTML-Datei) Links extrahieren\n",
    "        for _, row in df.iterrows():\n",
    "            try:  \n",
    "                result = extract_links_and_headlines(row)\n",
    "                results.append(result)\n",
    "                log_list.append({\n",
    "                    \"filename\": row[\"file_name\"],\n",
    "                    \"source\": row[\"name\"],\n",
    "                    \"date\": row[\"date\"],\n",
    "                    \"num_links\": len(result)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                failing_list.append({\n",
    "                    \"filename\": row[\"file_name\"],\n",
    "                    \"source\": row[\"name\"],\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FEHLER] Fehler beim Einlesen von {csv_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f73f59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Anzahl gefundener Links insgesamt: 5593\n",
      "[INFO] Verarbeitete Metadaten-Dateien: 1490\n",
      "[INFO] Links gespeichert unter: D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\\output\\artikel_links_headlines.csv\n"
     ]
    }
   ],
   "source": [
    "# Ergebnisse zusammenführen und anzeigen\n",
    "if results:\n",
    "    df_artikel = pd.concat(results, ignore_index=True)\n",
    "    print(\"[INFO] Anzahl gefundener Links insgesamt:\", len(df_artikel))\n",
    "\n",
    "    # Ergebnis als CSV-Datei exportieren\n",
    "    df_artikel.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"[INFO] Verarbeitete Metadaten-Dateien: {len(csv_files)}\")\n",
    "    print(f\"[INFO] Links gespeichert unter: {CSV_PATH}\")\n",
    "else:\n",
    "    print(\"[WARNUNG] Keine Artikellinks gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf65d5b",
   "metadata": {},
   "source": [
    "#### 4. Speicherung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47cbff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Fehlerhafte HTML-Dateien: 0\n",
      "[INFO] Fehlerhafte Links gespeichert unter: D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\\output\\artikel_links_failed.csv\n"
     ]
    }
   ],
   "source": [
    "# Fehlerliste in DataFrame umwandeln und als CSV-Datei exportieren\n",
    "pd.DataFrame(failing_list).to_csv(FAILED_PATH, index=False)\n",
    "print(f\"[INFO] Fehlerhafte HTML-Dateien: {len(failing_list)}\")\n",
    "print(f\"[INFO] Fehlerhafte Links gespeichert unter: {FAILED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e66c73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Erfolgreich verarbeitete HTML-Dateien: 33879\n",
      "[INFO] Log-Datei gespeichert unter: D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\\output\\artikel_links_log.csv\n"
     ]
    }
   ],
   "source": [
    "# Log-Datei in DataFrame umwandeln und als CSV-Datei exportieren\n",
    "pd.DataFrame(log_list).to_csv(LOG_PATH, index=False)\n",
    "print(f\"[INFO] Erfolgreich verarbeitete HTML-Dateien: {len(log_list)}\")\n",
    "print(f\"[INFO] Log-Datei gespeichert unter: {LOG_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
