{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593d0181",
   "metadata": {},
   "source": [
    "### Extraktion und Bereinigung der Pressetexte\n",
    "\n",
    "In diesem Notebook werden die Inhalte der BVG-Pressemitteilungen aus den HTML-Dateien extrahiert und für die anschließende Analyse aufbereitet.\n",
    "\n",
    "Die wichtigsten Schritte:\n",
    "- Sichtbare Pressetexte aus den HTML-Dateien extrahieren\n",
    "- Texte bereinigen und in einzelne Wörter zerlegen\n",
    "- Wortfrequenzen je Pressemitteilung zählen\n",
    "- Ergebnisse in einer SQLite-Datenbank und als CSV-Datei speichern\n",
    "\n",
    "Dieses Skript baut auf den gefilterten Pressemitteilungen im Betrachtungszeitraum auf (siehe: 07_pm_scraping_main.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960493ff",
   "metadata": {},
   "source": [
    "#### 1. Import benötigte Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a97dba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import os\n",
    "import pandas as pd # Datenanalyse\n",
    "\n",
    "# Automatisierte Datenübersicht\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Bearbeiten von HTML-Dateien\n",
    "from bs4 import BeautifulSoup  # HTML auslesen und bereinigen\n",
    "\n",
    "# Speicherung\n",
    "import sqlite3  # Speicherung in SQLite-Datenbanken\n",
    "\n",
    "# Eigene Funktionen (ausgelagert)\n",
    "import sys  # Systemfunktionen \n",
    "sys.path.append(\"..\") # Pfad zu .py Datei\n",
    "# Funktionen aus datenaufbereitung.py importieren\n",
    "from scripts.datenaufbereitung import (\n",
    "    load_stopwords,\n",
    "    read_html_file,\n",
    "    get_press_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d9aceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade \n",
    "# Projektverzeichnis \n",
    "PROJECT_ROOT = r\"D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\"\n",
    "\n",
    "# Eingabedaten: CSV-Datei mit gültigen Pressemitteilungen\n",
    "DATAPATH = os.path.join(PROJECT_ROOT, \"output\", \"pm_bvg_valid.csv\")\n",
    "\n",
    "# Input: # HTML-Dateien der Pressemitteilungen\n",
    "INPUT_PATH = os.path.join(PROJECT_ROOT, \"input\", \"pm_bvg_raw\")\n",
    "\n",
    "# Output \n",
    "OUTPUT_PATH = os.path.join(PROJECT_ROOT, \"output\")\n",
    "CSV_CLEAN_PATH = os.path.join(OUTPUT_PATH, \"pm_bvg_clean.csv\")\n",
    "SQL_CLEAN_PATH = os.path.join(OUTPUT_PATH, \"wordcount_pm_clean.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680887c",
   "metadata": {},
   "source": [
    "#### 2. Datenexploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe18cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV-Datei einlesen\n",
    "df_pm = pd.read_csv(DATAPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a22452cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>file_name</th>\n",
       "      <th>status</th>\n",
       "      <th>encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13-points-go-to</td>\n",
       "      <td>2021-08-06</td>\n",
       "      <td>2021</td>\n",
       "      <td>13-points-go-to.html</td>\n",
       "      <td>gültig</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140-jahre-unter-strom</td>\n",
       "      <td>2021-05-12</td>\n",
       "      <td>2021</td>\n",
       "      <td>140-jahre-unter-strom.html</td>\n",
       "      <td>gültig</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>155-000-mal-5</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>155-000-mal-5.html</td>\n",
       "      <td>gültig</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-03-pm-kleidersammelaktion0</td>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-01-03-pm-kleidersammelaktion0.html</td>\n",
       "      <td>gültig</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-08-pm-zahlen-2024</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>2025</td>\n",
       "      <td>2025-01-08-pm-zahlen-2024.html</td>\n",
       "      <td>gültig</td>\n",
       "      <td>utf-8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name        date  year  \\\n",
       "0                     13-points-go-to  2021-08-06  2021   \n",
       "1               140-jahre-unter-strom  2021-05-12  2021   \n",
       "2                       155-000-mal-5  2021-12-01  2021   \n",
       "3  2025-01-03-pm-kleidersammelaktion0  2025-01-03  2025   \n",
       "4           2025-01-08-pm-zahlen-2024  2025-01-08  2025   \n",
       "\n",
       "                                 file_name  status encoding  \n",
       "0                     13-points-go-to.html  gültig    utf-8  \n",
       "1               140-jahre-unter-strom.html  gültig    utf-8  \n",
       "2                       155-000-mal-5.html  gültig    utf-8  \n",
       "3  2025-01-03-pm-kleidersammelaktion0.html  gültig    utf-8  \n",
       "4           2025-01-08-pm-zahlen-2024.html  gültig    utf-8  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Überblick df_pm\n",
    "df_pm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4c6f3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(436, 6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anzahl Zeilen und Spalten\n",
    "df_pm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e467c6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9312efffa7c8466db0f6a4561b6c3dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 12.87it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20dd1f6561b4ec79e1e9b9ad82bda24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475c555b97db483f895d5d2b8025bcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91d3f0722e448df8a3828731dba2341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Report gespeichert unter: D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\\output\\rohdaten_bvg_profile_report.html\n"
     ]
    }
   ],
   "source": [
    "# Übersicht Daten\n",
    "# Data profiling \n",
    "profile = ProfileReport(df_pm)\n",
    "\n",
    "# Pfad\n",
    "profile_path = os.path.join(OUTPUT_PATH, \"rohdaten_bvg_profile_report.html\")\n",
    "\n",
    "# HTML-Export\n",
    "profile.to_file(profile_path)\n",
    "\n",
    "print(f\"[INFO] Report gespeichert unter: {profile_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f26071bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeitraum:\n",
      "Von: 2021-04-01\n",
      "Bis: 2025-04-28\n"
     ]
    }
   ],
   "source": [
    "# Zeitraum \n",
    "df_pm[\"date\"] = pd.to_datetime(df_pm[\"date\"])  \n",
    "\n",
    "print(\"Zeitraum:\")\n",
    "print(\"Von:\", df_pm[\"date\"].min().date())\n",
    "print(\"Bis:\", df_pm[\"date\"].max().date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90a4bc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name         0\n",
       "date         0\n",
       "year         0\n",
       "file_name    0\n",
       "status       0\n",
       "encoding     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fehlende Werte \n",
    "df_pm.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc9599",
   "metadata": {},
   "source": [
    "#### 3. Funktionen zur Verarbeitung definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "685d0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Eigentlicher Pressetext aus Pressemitteilungen extrahieren \n",
    "def extract_press_text(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # div suchen, das mit \"Page_additionalContent\" beginnt (ab da beginnt \"weitere Pressemitteilungen\")\n",
    "    stop_div = soup.find(\"div\", class_=lambda x: x and x.startswith(\"Page_additionalContent\"))\n",
    "\n",
    "    # Aktuelle Pressemitteilungen (mit <p>-Tags) bis stop_div sammeln\n",
    "    if stop_div:\n",
    "        paragraphs = stop_div.find_all_previous(\"p\")\n",
    "        # Richtige Reihenfolge wiederherstellen\n",
    "        paragraphs.reverse()\n",
    "        if paragraphs:\n",
    "            return \" \".join(p.get_text() for p in paragraphs)\n",
    "\n",
    "    # Ältere Pressemitteilungen (RichText-Klasse)\n",
    "    richtext_divs = soup.find_all(\"div\", class_=\"RichText_RichText__F_qRr\")\n",
    "    if richtext_divs:\n",
    "        return \" \".join(div.get_text() for div in richtext_divs)\n",
    "\n",
    "    # Fallback\n",
    "    return soup.get_text(separator=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f97c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Eigentlicher Pressetext in Wörter umwandeln, Stoppwörter entfernen\n",
    "def clean_and_split_text(text, stopwords_list):\n",
    "    # Kleinbuchstaben, Zeilenumbrüche raus, in Wörter aufteilen\n",
    "    text = text.lower().replace(\"\\n\", \" \")\n",
    "    tokens = text.split(\" \")\n",
    "    # Stoppwörter und kurze Wörter (<=1 Zeichen) entfernen\n",
    "    return [w for w in tokens if len(w) > 1 and w not in stopwords_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286ea60",
   "metadata": {},
   "source": [
    "#### 3. Anwendung der Verarbeitungsfunktionen auf die Pressemitteilungen  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b983c3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 1854 Stoppwörter geladen\n",
      "[INFO] Starte Verarbeitung von 436 Pressemitteilungen...\n"
     ]
    }
   ],
   "source": [
    "# Ergebnisse sammeln\n",
    "collection = []\n",
    "failed_html = []\n",
    "\n",
    "# Liste Stoppwörter laden\n",
    "stopwords_list = load_stopwords()\n",
    "print(f\"[INFO] {len(stopwords_list)} Stoppwörter geladen\")\n",
    "\n",
    "print(f\"[INFO] Starte Verarbeitung von {len(df_pm)} Pressemitteilungen...\")\n",
    "\n",
    "# Verarbeitung gültiger Pressemitteilungen (aus df_pm)\n",
    "for _, row in df_pm.iterrows():\n",
    "    file_name = row[\"file_name\"]\n",
    "    file_path = os.path.join(INPUT_PATH, row[\"file_name\"])\n",
    "    try:\n",
    "        # HTML laden\n",
    "        html = read_html_file(file_path, failed_html)\n",
    "\n",
    "        # Datum extrahieren (ausgelagerte Funktion)\n",
    "        date_str, year = get_press_date(html)  \n",
    "\n",
    "        # Pressetext extrahieren\n",
    "        text = extract_press_text(html)\n",
    "\n",
    "        # Bereinigen & in Wörter zerlegen\n",
    "        words = clean_and_split_text(text, stopwords_list)\n",
    "\n",
    "        # Wortfrequenz zählen\n",
    "        count = pd.Series(words).value_counts()\n",
    "\n",
    "        # DataFrame erstellen\n",
    "        count_df = count.to_frame()\n",
    "        count_df.columns = [\"count\"]\n",
    "        count_df[\"word\"] = count_df.index\n",
    "        count_df[\"file_name\"] = file_name\n",
    "        count_df[\"source\"] = \"bvg_pm\"\n",
    "        count_df[\"date\"] = date_str\n",
    "\n",
    "        # Speichern\n",
    "        collection.append(count_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Fehler bei Datei {row['file_name']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05f12b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle Ergebnisse zusammenführen\n",
    "df_counts = pd.concat(collection, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d415f84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54291, 5)\n",
      "Verarbeitet: 436 Dateien\n",
      "Gesamtzahl Wörter: 54291\n"
     ]
    }
   ],
   "source": [
    "# Ergebnis anzeigen\n",
    "print(df_counts.shape)\n",
    "print(f\"Verarbeitet: {len(df_pm)} Dateien\")\n",
    "print(f\"Gesamtzahl Wörter: {len(df_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3501c",
   "metadata": {},
   "source": [
    "#### 5. Speicherung der Ergebnisse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f1b1ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wortzählung Pressetexte als CSV gespeichert unter: D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\\output\\pm_bvg_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Als CSV-Datei speichern\n",
    "df_counts.to_csv(CSV_CLEAN_PATH, index=False)\n",
    "\n",
    "print(f\"Wortzählung Pressetexte als CSV gespeichert unter: {CSV_CLEAN_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3907fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wortzählung Pressetexte gespeichert unter: D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\\output\\wordcount_pm_clean.sqlite\n"
     ]
    }
   ],
   "source": [
    "# Als SQLite-Datenbank speichern\n",
    "conn = sqlite3.connect(SQL_CLEAN_PATH)\n",
    "df_counts.to_sql(\"wordcount_pm_clean\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Wortzählung Pressetexte gespeichert unter: {SQL_CLEAN_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
