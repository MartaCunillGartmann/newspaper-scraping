{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af582376",
   "metadata": {},
   "source": [
    "### Metadatenextraktion und Vorbereitung der Pressemitteilungen\n",
    "\n",
    "In diesem Notebook werden die Pressemitteilungen der BVG automatisiert gesammelt, zeitlich gefiltert und als Metadaten gespeichert.\n",
    "\n",
    "Die wichtigsten Schritte:\n",
    "- Webscraping der Pressemitteilungs-Links von der BVG-Webseite\n",
    "- Download der HTML-Seiten der einzelnen Mitteilungen\n",
    "- Extraktion des Veröffentlichungsdatums aus jeder HTML-Datei\n",
    "- Filterung der Pressemitteilungen nach dem definierten Betrachtungszeitraum (01.04.2021 – 30.04.2025)\n",
    "- Speicherung der gültigen Pressemitteilungen als Metadatensatz (CSV und SQLite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c793c",
   "metadata": {},
   "source": [
    "##### 1. Import der benötigten Pakete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62a8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import benötigte Pakete\n",
    "import os # Dateipfaden\n",
    "import pandas as pd # Tabellenverarbeitung (DataFrames)\n",
    "from glob import glob # Mehrere Dateien suchen\n",
    "from datetime import datetime # # Datumsverarbeitung\n",
    "\n",
    "# Webscraping mit Selenium\n",
    "from selenium import webdriver # Browser automatisch steuern\n",
    "import undetected_chromedriver as uc # Umgehung von Bot-Erkennungen\n",
    "from selenium.webdriver.common.by import By # Elemente auf Webseiten finden\n",
    "from selenium.webdriver.common.action_chains import ActionChains # Interaktionen auf Webseiten \n",
    "from selenium.webdriver.support.ui import WebDriverWait # Warten auf Ladeereignisse\n",
    "from selenium.webdriver.support import expected_conditions as EC # Bedingungen für Warten\n",
    "import time # Zeitpausen \n",
    "\n",
    "# Speicherung\n",
    "import sqlite3 # SQL-Datenbank\n",
    "\n",
    "# Eigene Funktionen (ausgelagert)\n",
    "import sys  # Systemfunktionen \n",
    "sys.path.append(\"..\") # Pfad zu .py Datei\n",
    "from scripts.datenaufbereitung import (\n",
    "    load_stopwords,\n",
    "    read_html_file,\n",
    "    process_html,\n",
    "    get_press_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e942633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade \n",
    "# Projektverzeichnis \n",
    "PROJECT_ROOT = r\"D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\"\n",
    "\n",
    "# Input \n",
    "INPUT_PATH = os.path.join(PROJECT_ROOT, \"input\", \"pm_bvg_raw\")\n",
    "CSV_PATH_INPUT = os.path.join(INPUT_PATH, \"pm_links.csv\") \n",
    "\n",
    "# Output\n",
    "OUTPUT_PATH = os.path.join(PROJECT_ROOT, \"output\")\n",
    "CSV_PATH_VALID = os.path.join(OUTPUT_PATH, \"pm_bvg_valid.csv\")\n",
    "SQL_PATH_VALID = os.path.join(OUTPUT_PATH, \"pm_bvg_valid.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed658d4",
   "metadata": {},
   "source": [
    "#### 2. Funktionen zur Verarbeitung definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e701db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Pressemitteilungs-Links per Webscraping sammeln\n",
    "def run_bvg_scraper():\n",
    "    print(\"Starte Webscraping...\")\n",
    "\n",
    "    # Browser starten und Startseite aufrufen\n",
    "    driver = uc.Chrome()\n",
    "    driver.get(\"https://www.bvg.de/de/unternehmen/medienportal/pressemitteilungen\")\n",
    "    time.sleep(5)  # Erste Seite komplett laden lassen\n",
    "\n",
    "    # Cookies akzeptieren (Didomi-Banner)\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"didomi-notice-agree-button\"))\n",
    "        )\n",
    "        cookie_button = driver.find_element(By.ID, \"didomi-notice-agree-button\")\n",
    "        driver.execute_script(\"arguments[0].click();\", cookie_button)\n",
    "        print(\"[INFO] Cookies akzeptiert\")\n",
    "        time.sleep(2)  # Kurze Pause nach Cookie-Klick\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Cookie konnte nicht geklickt werden: {e}\")\n",
    "\n",
    "    # Link-Sammlung starten, Seite für Seite durchgehen\n",
    "    all_links = set()\n",
    "    current_page = 1\n",
    "\n",
    "    while True:\n",
    "        time.sleep(3)  # Warten auf Seiteninhalt\n",
    "        try:\n",
    "            # Links auf der Seite sammeln\n",
    "            links = driver.find_elements(By.CSS_SELECTOR, \"a[href^='/de/unternehmen/medienportal/pressemitteilungen/']\")\n",
    "            for link in links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href:\n",
    "                    all_links.add(href)\n",
    "\n",
    "            # Versuchen, zur nächsten Seite zu klicken\n",
    "            next_page = str(current_page + 1)\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            next_button = wait.until(\n",
    "                EC.presence_of_element_located((By.XPATH, f\"//button[normalize-space(text())='{next_page}']\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "            time.sleep(1)\n",
    "            ActionChains(driver).move_to_element(next_button).click().perform()\n",
    "            current_page += 1\n",
    "            time.sleep(4)  # Warten nach dem Klick\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[INFO] Ende bei Seite {current_page}, keine weiteren Seiten gefunden\")\n",
    "            break\n",
    "\n",
    "    # Browser schließen\n",
    "    driver.quit()\n",
    "\n",
    "    # Metadaten-Logliste \n",
    "    log_list = []\n",
    "    for href in all_links:\n",
    "        slug = href.strip(\"/\").split(\"/\")[-1]\n",
    "        log_list.append({\n",
    "            \"name\": slug,\n",
    "            \"date\": None,\n",
    "            \"file_name\": None,\n",
    "            \"status\": \"ungelesen\",\n",
    "            \"encoding\": None\n",
    "        })\n",
    "\n",
    "    # DataFrame erstellen\n",
    "    df_links = pd.DataFrame(log_list)\n",
    "\n",
    "    # CSV speichern\n",
    "    df_links.to_csv(CSV_PATH_INPUT, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # Ergebnis anzeigen\n",
    "    print(f\"[INFO] {len(df_links)} Links gesammelt und gespeichert unter: {CSV_PATH_INPUT}\")\n",
    "\n",
    "    return df_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702a6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: HTML-Inhalte der Pressemitteilungen mit Selenium herunterladen\n",
    "def download_pm_pages(df_links, input_path, delay=2):\n",
    "    print(\"[INFO] Starte HTML-Download der Pressemitteilungen...\")\n",
    "\n",
    "    # Browser starten \n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Neue Logliste für aktualisierte Metadaten\n",
    "    log_list = []\n",
    "    erfolgreich = 0\n",
    "    fehlerhaft = 0\n",
    "\n",
    "    # Alle Links durchlaufen\n",
    "    for index, row in df_links.iterrows():\n",
    "        url = f\"https://www.bvg.de/de/unternehmen/medienportal/pressemitteilungen/{row['name']}\"\n",
    "        try:\n",
    "            # HTML laden\n",
    "            driver.get(url)\n",
    "\n",
    "            # Nach dem Laden: Cookies akzeptieren\n",
    "            try:\n",
    "                WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.ID, \"didomi-notice-agree-button\"))\n",
    "                )\n",
    "                cookie_button = driver.find_element(By.ID, \"didomi-notice-agree-button\")\n",
    "                driver.execute_script(\"arguments[0].click();\", cookie_button)\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                # Beim ersten Eintrag in der Schleife Meldung anzeigen\n",
    "                if index == 0:\n",
    "                    print(\"[INFO] Cookie-Banner erkannt und akzeptiert\")\n",
    "\n",
    "            time.sleep(delay)\n",
    "            html = driver.page_source\n",
    "\n",
    "            # Dateiname aus URL\n",
    "            slug = url.strip(\"/\").split(\"/\")[-1]\n",
    "            filename = f\"{slug}.html\"\n",
    "            file_path = os.path.join(input_path, filename)\n",
    "\n",
    "            # HTML speichern\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html)\n",
    "\n",
    "            # Metadaten-Logliste \n",
    "            log_list.append({\n",
    "                \"name\": slug,\n",
    "                \"date\": None,\n",
    "                \"file_name\": filename,\n",
    "                \"status\": \"erfolgreich\",\n",
    "                \"encoding\": \"utf-8\"\n",
    "            })\n",
    "\n",
    "            erfolgreich += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            log_list.append({\n",
    "                \"name\": None,\n",
    "                \"date\": None,\n",
    "                \"file_name\": None,\n",
    "                \"status\": f\"Fehler: {e}\",\n",
    "                \"encoding\": None\n",
    "            })\n",
    "\n",
    "            fehlerhaft += 1\n",
    "\n",
    "    # Browser schließen\n",
    "    driver.quit()\n",
    "\n",
    "    # Aktualisierte Metadaten als DataFrame\n",
    "    df_links_updated = pd.DataFrame(log_list)\n",
    "\n",
    "    # CSV speichern\n",
    "    df_links_updated.to_csv(CSV_PATH_INPUT, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # Ergebnis anzeigen\n",
    "    print(f\"[INFO] HTML-Download abgeschlossen. CSV gespeichert unter: {CSV_PATH_INPUT}\")\n",
    "    print(f\"[INFO] Erfolgreich gespeichert: {erfolgreich}\")\n",
    "    print(f\"[INFO] Fehlerhafte Seiten: {fehlerhaft}\")\n",
    "\n",
    "    return df_links_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29f4d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Einzelne Pressemitteilungsdatei verarbeiten \n",
    "\n",
    "# Stoppwörter laden\n",
    "stopwords_list = load_stopwords()\n",
    "\n",
    "def process_press_release(press_release, input_path, failed_html):\n",
    "    # Dateiname holen\n",
    "    filename = os.path.basename(press_release[\"file_name\"])\n",
    "    full_path = os.path.join(input_path, filename)\n",
    "    \n",
    "    # HTML-Datei einlesen und Text in Wörter zerlegen (ausgelagerte Funktionen) \n",
    "    html = read_html_file(full_path, failed_html, encoding=\"utf-8\")\n",
    "    items = process_html(html, stopwords_list)\n",
    "\n",
    "    # Wortfrequenzen berechnen und als DataFrame speichern\n",
    "    count = pd.Series(items).value_counts()\n",
    "\n",
    "    count_df = count.to_frame()\n",
    "    count_df.columns = [\"count\"]\n",
    "    count_df[\"word\"] = count_df.index\n",
    "    count_df[\"source\"] = \"bvg_pm\"\n",
    "    count_df[\"date\"] = press_release[\"date\"]\n",
    "    \n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee63f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Pressemitteilungen für den Betrachtungszeitraum filtern\n",
    "def extract_valid_pm(input_path):\n",
    "\n",
    "    # HTML-Dateien suchen\n",
    "    files = glob(os.path.join(input_path, \"*.html\"))\n",
    "    \n",
    "    # Ergebnisliste und Zähler\n",
    "    log_list = []\n",
    "    gültig = 0\n",
    "    ignoriert = 0\n",
    "    fehlgeschlagen = 0\n",
    "\n",
    "    # Betrachtungszeitraum definieren\n",
    "    start_date = datetime(2021, 4, 1)\n",
    "    end_date = datetime(2025, 4, 30)\n",
    "\n",
    "    # Alle Dateien durchlaufen\n",
    "    for file_path in files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        try:\n",
    "            # HTML-Inhalt lesen\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                html = f.read()\n",
    "\n",
    "            # Veröffentlichungsdatum aus HTML-Dateien extrahieren (ausgelagerte Funktion)\n",
    "            date_str, year = get_press_date(html)\n",
    "\n",
    "            # Prüfung, ob Datum im Betrachtungszeitraum\n",
    "            if date_str:\n",
    "                date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "                if start_date <= date_obj <= end_date:\n",
    "                    log_list.append({\n",
    "                        \"name\": filename.replace(\".html\", \"\"),\n",
    "                        \"date\": date_str,\n",
    "                        \"year\": year,\n",
    "                        \"file_name\": filename,\n",
    "                        \"status\": \"gültig\",\n",
    "                        \"encoding\": \"utf-8\"\n",
    "                    })\n",
    "                    gültig += 1\n",
    "                else:\n",
    "                    ignoriert += 1 # außerhalb des Zeitraums\n",
    "            else:\n",
    "                fehlgeschlagen += 1 # kein Datum gefunden\n",
    "\n",
    "        except Exception as e:\n",
    "            fehlgeschlagen += 1\n",
    "\n",
    "    # Ergebnis anzeigen\n",
    "    print(f\"[INFO] {gültig} gültige Pressemitteilungen extrahiert\")\n",
    "    print(f\"[INFO] {ignoriert} ignoriert, {fehlgeschlagen} fehlgeschlagen\")\n",
    "\n",
    "    return log_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8ef32",
   "metadata": {},
   "source": [
    "#### 3. Iteratives Anwenden der Verarbeitungsfunktionen auf die Pressemitteilungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73803f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Webscraping...\n",
      "[INFO] Cookies akzeptiert\n",
      "[INFO] Ende bei Seite 79, keine weiteren Seiten gefunden\n",
      "[INFO] 944 Links gesammelt und gespeichert unter: D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\\input\\pm_bvg_raw\\pm_links.csv\n"
     ]
    }
   ],
   "source": [
    "# Webscraping: Pressemitteilungs-Links sammeln\n",
    "df_links = run_bvg_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b884298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starte HTML-Download der Pressemitteilungen...\n",
      "[INFO] HTML-Download abgeschlossen. CSV gespeichert unter: D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\\input\\pm_bvg_raw\\pm_links.csv\n",
      "[INFO] Erfolgreich gespeichert: 944\n",
      "[INFO] Fehlerhafte Seiten: 0\n"
     ]
    }
   ],
   "source": [
    "# HTML-Dateien der Pressemitteilungen herunterladen\n",
    "df_links = download_pm_pages(df_links, INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58d38825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 436 gültige Pressemitteilungen extrahiert\n",
      "[INFO] 508 ignoriert, 0 fehlgeschlagen\n"
     ]
    }
   ],
   "source": [
    "# Datum extrahieren und Pressemitteilungen für den Betrachtungszeitraum filtern (ausgelagerte Funktion anwenden)\n",
    "log_list = extract_valid_pm(INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b77df7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Als DataFrame speichern\n",
    "df_pm = pd.DataFrame(log_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c59dc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status\n",
      "gültig    436\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ergebnis anzeigen\n",
    "print(df_pm[\"status\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76169615",
   "metadata": {},
   "source": [
    "#### 4. Speicherung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94acf0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Als CSV gespeichert unter D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\\output\\pm_bvg_valid.csv\n"
     ]
    }
   ],
   "source": [
    "# DataFrame als CSV exportieren\n",
    "df_pm.to_csv(CSV_PATH_VALID, index=False, encoding=\"utf-8\")\n",
    "print(f\"[INFO] Als CSV gespeichert unter {CSV_PATH_VALID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "120b3066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] In Datenbank gespeichert unter D:/DBU/ADSC11 ADS-01/Studienarbeit/newspaper-scraping\\output\\pm_bvg_valid.sqlite\n"
     ]
    }
   ],
   "source": [
    "# In SQLite-Datenbank speichern\n",
    "conn = sqlite3.connect(SQL_PATH_VALID)\n",
    "df_pm.to_sql(\"pm_bvg_valid\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Verbindung schließen\n",
    "conn.close()\n",
    "print(f\"[INFO] In Datenbank gespeichert unter {SQL_PATH_VALID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
