
# Best Practices fÃ¼r dein ADS-01 Newspaper Scraping Projekt

## ğŸ“ Projektstruktur

- `input/` â€“ Eingabedaten (z.â€¯B. HTML-Dateien, CSVs)
- `output/` â€“ Zwischenergebnisse, bereinigte Daten etc.
- `scripts/` â€“ Python-Skripte (z.â€¯B. `merge.py`, `cleaning.py`)
- `lib_py/` â€“ ausgelagerte Funktionen, Hilfsfunktionen
- `tests/` â€“ Unit Tests mit `pytest`
- `docs/` â€“ Dokumentation (z.â€¯B. README, Markdown-Notizen)
- `notebooks/` â€“ Jupyter Notebooks fÃ¼r Exploratives

## ğŸ§ª Testing mit `pytest`

- Test-Dateien z.â€¯B. `test_merge.py`
- Testfunktionen beginnen mit `test_`
- Verwende `assert`, um Verhalten zu testen
- Optional: `tests/input/` fÃ¼r kleine Test-Daten

## ğŸ““ Jupyter Notebooks

- **Zelle 1**: Imports
- **Zelle 2**: Pfad-Konstanten, z.â€¯B.:

```python
FILENAME = "input/raw_data.csv"
```

- Danach: Nutzung dieser Konstanten im gesamten Notebook
- Code auslagern in `scripts/` und importieren

## ğŸ§  Code-Organisation

- `if __name__ == "__main__":` fÃ¼r `.py`-Dateien
- Funktionen mit Docstrings dokumentieren
- Kein Copy-Paste in vielen Dateien

## ğŸ§° Technisches Setup

- Nutzung von `pipenv`
- virtuelle Umgebung Ã¼ber `Pipfile`, `Pipfile.lock`
- `pipenv shell` zum Aktivieren
- `.gitignore` enthÃ¤lt `.venv/`, `__pycache__/`, `*.pyc`

## ğŸŒ Webscraping

- HTML-Struktur analysieren mit Rechtsklick â†’ â€Untersuchenâ€œ
- Relevante Tags: `<h1>`, `<h2>`, `<p>`, `<a>`, `<div>`
- Nutze `BeautifulSoup`, um gezielt Elemente zu finden
- Clean Code: Trennung von Parsing und Speicherung

