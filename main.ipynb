{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c63b0cd8-f07a-4d08-917e-e19168fe12d7",
   "metadata": {},
   "source": [
    "# main.ipynb – Datenextraktion & Vorverarbeitung\n",
    "In diesem Notebook wird die strukturierte Verarbeitung von Online-Medienbeiträgen vorbereitet.  \n",
    "Die wichtigsten Schritte:\n",
    "- `.tar.gz`-Dateien entpacken (HTML-Dateien extrahieren)\n",
    "- HTML-Inhalte bereinigen und Stoppwörter entfernen\n",
    "- Wortfrequenzen je Medium und Datum zählen\n",
    "- Ergebnisse in einer SQLite-Datenbank (`output/dwh.sqlite3`) speichern\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f96df4ba-a813-4e82-a36e-4149963bd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benötigte Pakete importieren\n",
    "# \"Standard\"-import\n",
    "import os\n",
    "import pandas as pd\n",
    "# Einlesen & entpacken in html-Dateien\n",
    "from glob import glob\n",
    "import tarfile\n",
    "# Kreation Mini-Data-Warehouse (SQL-Datenbank)\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "# Bearbeiten von html-Dateien\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9966097-f518-4e15-a890-ce37773747dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfade für das Projekt \n",
    "\n",
    "# Pfad für das Projekt\n",
    "PROJECT_ROOT = os.getcwd()  \n",
    "\n",
    "# Input, Rohdaten & html-Dateien\n",
    "INPUT_PATH = os.path.join(PROJECT_ROOT, \"input\", \"raw\")\n",
    "ZIP_PATH = os.path.join(INPUT_PATH, \"downloaded_zips\")\n",
    "DATA_LAKE_PATH = os.path.join(INPUT_PATH, \"data-lake\")  \n",
    "\n",
    "# Mini-Data-Warehouse bzw. SQL-Datenbank\n",
    "LOGFILE_PATH = os.path.join(DATA_LAKE_PATH, \"2025-02-08.csv\")\n",
    "log_file = pd.read_csv(LOGFILE_PATH)\n",
    "SQL_PATH = os.path.join(PROJECT_ROOT, \"output\", \"dwh.sqlite3\")\n",
    "\n",
    "# Stoppwörter einmalig laden\n",
    "STOPWORDS_URL = \"https://raw.githubusercontent.com/solariz/german_stopwords/master/german_stopwords_full.txt\"\n",
    "stopwords_list = requests.get(STOPWORDS_URL, allow_redirects=True).text.split(\"\\n\")[9:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f7a4d13-fadf-49c1-8e49-e315bd3b314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahieren & Einlesen einer .tar.gz-Datei\n",
    "def extract_tar_file(ZIP_PATH, DATA_LAKE_PATH): \n",
    "    with tarfile.open(ZIP_PATH, \"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            # Zielpfad für jede Datei bestimmen\n",
    "            filename = os.path.basename(member.name)\n",
    "            \n",
    "            # Nur HTML-Dateien entpacken\n",
    "            if filename.endswith(\".html\"):\n",
    "                target_path = os.path.join(DATA_LAKE_PATH, filename)\n",
    "                \n",
    "                # Skippen, wenn Datei schon existiert\n",
    "                if os.path.exists(target_path):\n",
    "                    continue\n",
    "                \n",
    "                # Entpacken in HTML-Datei\n",
    "                with open(target_path, \"wb\") as f:\n",
    "                    f.write(tar.extractfile(member).read())\n",
    "                \n",
    "                print(f\"Entpackt: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c517cf6b-8ac3-4333-bce7-8d70fd6c7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html-Datei mit Encoding einlesen\n",
    "def read_html_file(filepath, encoding=\"utf-8\"):\n",
    "    # Datei öffnen und einlesen\n",
    "    with open(filepath, \"r\", encoding=encoding) as f:\n",
    "        return f.read()\n",
    "\n",
    "# html-Inhalt bereinigen & in einzelne Wörter zerlegen\n",
    "def process_html(html):\n",
    "    # html-Parser von BeautifulSoup nutzen, um nur den Text auszulesen\n",
    "    bstext = BeautifulSoup(html, \"html.parser\")\n",
    "    text = bstext.get_text(separator=\" \").lower()\n",
    "\n",
    "    # Zeilenumbrüche durch Leerzeichen ersetzen & Wörter trennen\n",
    "    items = text.replace(\"\\n\", \" \").split(\" \")\n",
    "\n",
    "    # Kurze Wörter und Stoppwörter entfernen\n",
    "    items = [i for i in items if len(i) > 1 and i not in stopwords_list]\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7453595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einzelne Zeitungsdateien verarbeiten \n",
    "def process_newspaper(newspaper):\n",
    "    filename = os.path.basename(newspaper[\"file_name\"])  # Nur den Dateinamen rausholen\n",
    "    full_path = os.path.join(DATA_LAKE_PATH, filename)   \n",
    "\n",
    "    # Encoding aus der CSV einlesen\n",
    "    encoding = newspaper[\"encoding\"].lower()\n",
    "    \n",
    "    # html-Datei laden & bereinigen\n",
    "    html = read_html_file(full_path, encoding)\n",
    "    items = process_html(html)\n",
    "\n",
    "   # Häufigkeit jedes Wortes zählen\n",
    "    count = pd.Series(items).value_counts()\n",
    "\n",
    "    # Als DataFrame formatieren und Zusatzinfos ergänzen\n",
    "    count_df = count.to_frame()\n",
    "    count_df.columns = [\"count\"]\n",
    "    count_df[\"word\"] = count_df.index\n",
    "    count_df[\"paper\"] = newspaper[\"name\"]\n",
    "    count_df[\"date\"] = newspaper[\"date\"]\n",
    "\n",
    "    return count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ed4b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste für die Ergebnisse-Sammlung erstellen\n",
    "collection = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a70183c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verarbeitung jeder einzelnen Datei und DataFrame mit Wortzählungen zurückgeben\n",
    "def process_wrapper(newspaper):\n",
    "    name = newspaper[\"name\"]\n",
    "    try:\n",
    "        count = process_newspaper(newspaper)\n",
    "        print(f\"[INFO] Verarbeitung erfolgreich: {name}\")\n",
    "        collection.append(count)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Fehler bei {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "229771e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Verarbeitung erfolgreich: sz\n",
      "[INFO] Verarbeitung erfolgreich: zeit\n",
      "[INFO] Verarbeitung erfolgreich: faz\n",
      "[INFO] Verarbeitung erfolgreich: heise\n",
      "[INFO] Verarbeitung erfolgreich: golem\n",
      "[INFO] Verarbeitung erfolgreich: tagesspiegel\n",
      "[INFO] Verarbeitung erfolgreich: taz\n",
      "[INFO] Verarbeitung erfolgreich: abendblatt\n",
      "[INFO] Verarbeitung erfolgreich: berliner\n",
      "[INFO] Verarbeitung erfolgreich: welt\n",
      "[INFO] Verarbeitung erfolgreich: medium\n",
      "[INFO] Verarbeitung erfolgreich: esslinger\n",
      "[INFO] Verarbeitung erfolgreich: kdnuggets\n",
      "[INFO] Verarbeitung erfolgreich: handelsblatt\n",
      "[INFO] Verarbeitung erfolgreich: ntv\n",
      "[INFO] Verarbeitung erfolgreich: pioneer\n",
      "[INFO] Verarbeitung erfolgreich: suedwest\n",
      "[INFO] Verarbeitung erfolgreich: t3n\n",
      "[INFO] Verarbeitung erfolgreich: economist\n",
      "[INFO] Verarbeitung erfolgreich: srf\n",
      "[INFO] Verarbeitung erfolgreich: wef\n",
      "[INFO] Verarbeitung erfolgreich: stuttgarter\n",
      "[INFO] Verarbeitung erfolgreich: atlantic\n",
      "[INFO] Verarbeitung erfolgreich: netzpolitik\n",
      "[INFO] Verarbeitung erfolgreich: towardsds\n",
      "[INFO] Verarbeitung erfolgreich: uebermedien\n",
      "[INFO] Verarbeitung erfolgreich: vulture\n",
      "[INFO] Verarbeitung erfolgreich: boerse\n",
      "[INFO] Verarbeitung erfolgreich: buchreport\n",
      "[INFO] Verarbeitung erfolgreich: 54books\n",
      "[INFO] Verarbeitung erfolgreich: dlf\n",
      "[INFO] Verarbeitung erfolgreich: dw-de\n",
      "[INFO] Verarbeitung erfolgreich: spiegel\n",
      "[INFO] Verarbeitung erfolgreich: cnn\n",
      "[INFO] Verarbeitung erfolgreich: bbc\n",
      "[INFO] Verarbeitung erfolgreich: dhv\n",
      "[INFO] Verarbeitung erfolgreich: mm\n",
      "[INFO] Verarbeitung erfolgreich: stern\n",
      "[INFO] Verarbeitung erfolgreich: tagesschau\n",
      "[INFO] Verarbeitung erfolgreich: zvw\n",
      "[INFO] Verarbeitung erfolgreich: ieee\n",
      "[INFO] Verarbeitung erfolgreich: anwaltsverein\n",
      "[INFO] Verarbeitung erfolgreich: wiwo\n",
      "[INFO] Verarbeitung erfolgreich: dw-en\n",
      "[INFO] Verarbeitung erfolgreich: dav\n",
      "[INFO] Verarbeitung erfolgreich: zwanzig\n",
      "[INFO] Verarbeitung erfolgreich: blick\n",
      "[INFO] Verarbeitung erfolgreich: nzz\n",
      "[INFO] Verarbeitung erfolgreich: republik\n",
      "[INFO] Verarbeitung erfolgreich: ta\n",
      "[INFO] Verarbeitung erfolgreich: watson-ch\n",
      "[INFO] Verarbeitung erfolgreich: nau\n",
      "[INFO] Verarbeitung erfolgreich: standard\n",
      "[INFO] Verarbeitung erfolgreich: kronen\n",
      "[INFO] Verarbeitung erfolgreich: kurier\n",
      "[INFO] Verarbeitung erfolgreich: kleine\n",
      "[INFO] Verarbeitung erfolgreich: watson-de\n",
      "[INFO] Verarbeitung erfolgreich: vice-de\n",
      "Data shape: (53939, 4)\n"
     ]
    }
   ],
   "source": [
    "# Verarbeitung jeder Zeile\n",
    "log_file.apply(process_wrapper, axis=1)\n",
    "\n",
    "# Alle Ergebnisse in einer Tabelle sammeln\n",
    "data = pd.concat(collection, axis=0)\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec1d17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Datenbank speichern\n",
    "# Erstellung SQL-Datenbank (dwh.sqlite3)\n",
    "connection = sqlite3.connect(SQL_PATH)\n",
    "\n",
    "# Wordcount-DataFrame in eine Tabelle erstellen\n",
    "data.to_sql(\"wordcount\", connection, index=False, if_exists=\"append\")\n",
    "\n",
    "# SQL-Datenbank schließen, um Speicherplatz freizugeben\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newspaper-scraping-08G4y0J9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
